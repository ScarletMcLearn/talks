{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kategorien für Gastronomie-Feedbacks ableiten\n",
    "\n",
    "[TeLLers](https://tellers.co.at/) ist eine mobile Web-Applikation um Feedback für Gastronomiebetriebe zu sammeln. Im Unterschied zu zahlreichen Web-Seiten mit einem ähnlichen Ziel sind diese Feedbacks nicht im Internet öffentlich zugänglich. Stattdessen kann sie nur der Gastronom selbst einsehen und damit gezielt seine Produkte und Dienstleistungen den Kundenbedürfnissen anpassen, ohne sich in öffentliche Diskussionen und potentielle \"Shot storms\" zu verstricken.\n",
    "\n",
    "Dieses Notebook zeigt, wie bestehende Feedback-Einträge analysiert werden können, um darauf Aufsetzend ein Kategoriesystem zu entwickeln."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Überblick\n",
    "\n",
    "Die Feedbackdaten liegen in einer SQLite-Tabelle `feedback` in der Spalte `text`. Unter verwendung von Python 3.5+ landen die Daten in einer Liste, werden mit Funktionen aus der der Bibliothek [nltk](https://www.nltk.org/) in Sätze und Wörter zerlegt und dann mit [gensim](https://radimrehurek.com/gensim/) nach darin enthaltenen Themen untersucht. Die vorliegende Analyse basiert auf dem Gensim-Tutorial zu \"[Topics and Transformations](https://radimrehurek.com/gensim/tut2.html)\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorbereitung\n",
    "\n",
    "Um Log-Ausgaben übersichtlich darzustellen sind folgende Einstellungen sinnvoll:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folgende Funktion wird später verwendet, um eine Liste von Listen von Wörtern um leere Wortlisten zu bereinigen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def clean_texts(texts: List[List[str]]) -> List[List[str]]:\n",
    "    text_index = 0\n",
    "    while len(texts) > text_index:\n",
    "        if len(texts[text_index]) == 0:\n",
    "            del texts[text_index]\n",
    "        else:\n",
    "            text_index += 1\n",
    "    logging.info('%d texts remaining', len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import gensim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback-Texte einlesen\n",
    "\n",
    "Da die Daten als SQLite-Datenbank aufliegen (Tabelle bzw. Feld `feedback.text`) lassen sie sich wie folgt in eine Python-Liste `feedback_texts` übertragen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : found 562 feedbacks\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "from contextlib import closing\n",
    "\n",
    "with sqlite3.connect('/tmp/tellers.db') as tellers_db:\n",
    "    with closing(tellers_db.cursor()) as tellers_cursor:\n",
    "        tellers_cursor.execute(\"\"\"\n",
    "            select \n",
    "                text \n",
    "            from \n",
    "                feedback \n",
    "            where 1 = 1\n",
    "                and source_id = 1  -- consider only TeLLers feedback\n",
    "                and length(text) >= 20\n",
    "                and lower(text) not like '%test%'\n",
    "            order by\n",
    "                feedback_time desc  -- newer feedback first\n",
    "        \"\"\")\n",
    "        feedback_texts = [row[0] for row in tellers_cursor.fetchall()]\n",
    "logging.info('found %d feedbacks', len(feedback_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Ergebnis ist eine Liste wie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Freundliche Bedingung.\\\\nGute Qualität der Speisen.',\n",
       " 'Das Interior modernisieren',\n",
       " 'Eine vegane Torte oder ein Veganer Kuchen']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedback_texts[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Texte in Sätze und Wörter zerlegen\n",
    "\n",
    "Im ersten Schritte sind die potentiell langen Feedbacks in Sätze zu zerlegen. Dazu ist die Funktion `nltk.sent_tokenize()` hilfreich, da sie auch mit fortgeschrittenen Konzepten wie der indirekten Sprache umgehen kann:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : found 904 feedback sentences\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "sentences = []\n",
    "for feedback_text in feedback_texts:\n",
    "    feedback_sentences = nltk.sent_tokenize(feedback_text)\n",
    "    sentences.extend(feedback_sentences)\n",
    "logging.info('found %d feedback sentences', len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dokumente in Text-Token umwandeln:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : found 904 texts with words\n"
     ]
    }
   ],
   "source": [
    "texts = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "logging.info('found %d texts with words', len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nur mit Großbuchstaben beginnende Wörte behalten (obsolet)\n",
    "\n",
    "Um in vereinfachter Form nur Hauptwörter für die Analyse zu behalten, ist eine pragmatische Abkürzung, die Texte auf Wörter zu reduzieren, die mit Großbuchstaben beginnen. Dies ist allerdings nicht ganz korrekt, da auch der Satzanfang oder Substantivierungen mit Großbuchstaben beginnen.\n",
    "\n",
    "> Hinweis: da dieser Filter die Qualität der Analyse eher nagativ beeinflusst, wurde er wieder deaktiviert. Da er für andere Anwendungen sinnvoll sein kann, ist der deaktivierte Code im folgenden dennoch angeführt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    texts = [[word for word in text if word[0].upper() == word[0]]\n",
    "             for text in texts]\n",
    "    clean_texts(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nur Text-Wörter behalten und in Kleinschrift umwandeln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um Satzzeichen und Zahlen zu entfernen, bietet es such an, nur jene Wörter zu behalten, die mit einem Buchstaben beginnen. Hilfreich dazu ist die Funktionen `str.isalpha()`, die übrigens auch Umlaute erkennt. Im Zuges dieses Schritts erfolgt auch eine Umwandlung der Wörter in Kleinschrift, da die darauffolgenden Filter damit einfacher umzusetzen sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : 868 texts remaining\n"
     ]
    }
   ],
   "source": [
    "texts = [[word.lower() for word in text if word[0].isalpha()]\n",
    "         for text in texts]\n",
    "clean_texts(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stoppwörter entfernen\n",
    "\n",
    "Stoppwörter sind Wörter, welche für die Analyse keine Relevanz haben, zum Beispiel bestimmte Artikel wie \"der\" oder Bindewörter wie \"oder\". Das Projekt [stopwords-iso](https://github.com/stopwords-iso/stopwords-iso) sammelt Stoppwörter für verschiedene Sprachen. Mit Hilfe des [requests](http://docs.python-requests.org/)-Modules ist es kompakt mögliche, die aktuellste Version der deutschen Stoppwörter eine eine Python-Menge (`set`) zu übertragen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'ab', 'aber', 'ach', 'acht', 'achte', 'achten', 'achter', 'achtes', 'ag']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "query = requests.get('https://raw.githubusercontent.com/stopwords-iso/stopwords-de/master/stopwords-de.txt')\n",
    "query.raise_for_status()\n",
    "stopwords = set(query.text.split('\\n'))\n",
    "print(sorted(stopwords)[:10])  # Print inital 10 stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit können die Stoppwörter aus den Feedback-Texten entfernt werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : 849 texts remaining\n"
     ]
    }
   ],
   "source": [
    "texts = [[word for word in text if not word in stopwords]\n",
    "         for text in texts]\n",
    "clean_texts(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nur einmal vorkommende Wörter entfernen\n",
    "\n",
    "Wörter, die in allen Feedbacks nur einmal vorkommen, sind so speziell, dass sie auf das Ergebnis keinen Einfluss haben. Um die Performanze zu verbessern, kann man diese noch vor der Analyse entfernen. Die Standard-Module `collections` und `iterable` enthalten dazu hilfreiche Funktionen, um eine Liste von Liste zu \"verflachen\" und die Häufigkeit der darin enthaltenene Texte zu ermitteln."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : 751 texts remaining\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "word_counter = Counter(chain.from_iterable(texts))\n",
    "once_words = set(word for word, count in word_counter.items() if count == 1)\n",
    "texts = [[word for word in text if word not in once_words]\n",
    "         for text in texts]\n",
    "clean_texts(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zwischenergebnis\n",
    "\n",
    "Als Ergenis der bisherigen Umwandlungen eralten wir eine Liste von Wortlisten, die Beispielhaft wie folgt aussieht:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['freundliche', 'qualität', 'speisen'],\n",
       " ['modernisieren'],\n",
       " ['vegane', 'torte', 'veganer'],\n",
       " ['super', 'restaurant'],\n",
       " ['gedicht'],\n",
       " ['essen', 'super', 'schnell'],\n",
       " ['gerne'],\n",
       " ['bestens'],\n",
       " ['eventuell', 'pizza'],\n",
       " ['danke', 'gerne', 'trotz', 'freundliche', 'bedienung']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse mit gensim\n",
    "\n",
    "Um die nun als Wortlisten vorliegenden Texte zu mit gensim analysieren, sind mehrere Schritte erforderlich.\n",
    "\n",
    "Zuerste wird daraus ein Wörterbuch für den Corpus erstellt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO : built Dictionary(474 unique tokens: ['freundliche', 'qualität', 'speisen', 'modernisieren', 'torte']...) from 751 documents (total 1930 corpus positions)\n",
      "INFO : saving Dictionary object under /tmp/gastro_feedback.dict, separately None\n",
      "INFO : saved /tmp/gastro_feedback.dict\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.save('/tmp/gastro_feedback.dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit lässt sich der tatsächliche Corpus aufbauen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : storing corpus in Matrix Market format to /tmp/deerwester.mm\n",
      "INFO : saving sparse matrix to /tmp/deerwester.mm\n",
      "INFO : PROGRESS: saving document #0\n",
      "INFO : saved 751x474 matrix, density=0.531% (1892/355974)\n",
      "INFO : saving MmCorpus index to /tmp/deerwester.mm.index\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('/tmp/deerwester.mm', corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aus dem Corpus lässt sich die \"[term frequency–inverse document frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) (TFIDF) berechnen. Dabei handelt es sich um eine Kennzahl, die jedem Wort eine Wichtigkeit zuordnet. Basis dafür ist, wie oft das Wort in einem Feedback-Text und wie oft es im gesamten Corpus auftritt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : collecting document frequencies\n",
      "INFO : PROGRESS: processing document #0\n",
      "INFO : calculating IDF weights for 751 documents and 473 features (1892 matrix non-zeros)\n"
     ]
    }
   ],
   "source": [
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der so transformierte Corpus kann nun als Basis für einen \"[latent semantic index](https://en.wikipedia.org/wiki/Latent_semantic_analysis)\" (LSI) dienen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : using serial LSI version on this node\n",
      "INFO : updating model with new documents\n",
      "INFO : preparing a new chunk of documents\n",
      "INFO : using 100 extra samples and 2 power iterations\n",
      "INFO : 1st phase: constructing (474, 106) action matrix\n",
      "INFO : orthonormalizing (474, 106) action matrix\n",
      "INFO : 2nd phase: running dense svd on (106, 751) matrix\n",
      "INFO : computing the final decomposition\n",
      "INFO : keeping 6 factors (discarding 83.464% of energy spectrum)\n",
      "INFO : processed documents up to #751\n",
      "INFO : topic #0(4.002): -0.789*\"super\" + -0.459*\"essen\" + -0.214*\"kellner\" + -0.174*\"personal\" + -0.102*\"musik\" + -0.093*\"bedienung\" + -0.074*\"top\" + -0.073*\"echt\" + -0.064*\"qualität\" + -0.063*\"lecker\"\n",
      "INFO : topic #1(3.669): -0.836*\"essen\" + 0.439*\"super\" + 0.210*\"kellner\" + -0.087*\"top\" + -0.076*\"ambiente\" + -0.074*\"schmeckt\" + 0.072*\"musik\" + 0.055*\"personal\" + -0.051*\"tolle\" + -0.050*\"teuer\"\n",
      "INFO : topic #2(3.252): 0.904*\"kellner\" + -0.268*\"super\" + -0.168*\"personal\" + 0.155*\"blonde\" + 0.081*\"essen\" + 0.061*\"bedienung\" + -0.060*\"lokal\" + 0.054*\"netter\" + 0.053*\"könnten\" + 0.048*\"is\"\n",
      "INFO : topic #3(3.052): 0.732*\"personal\" + 0.512*\"lokal\" + -0.239*\"super\" + 0.173*\"musik\" + 0.114*\"getränke\" + 0.107*\"freundliches\" + 0.086*\"kellner\" + 0.079*\"angebot\" + 0.072*\"tisch\" + 0.062*\"bedienung\"\n",
      "INFO : topic #4(2.924): -0.802*\"musik\" + -0.241*\"top\" + -0.217*\"laut\" + -0.199*\"gerne\" + -0.190*\"perfekt\" + 0.172*\"personal\" + 0.147*\"lokal\" + 0.105*\"kellner\" + -0.102*\"unterhalten\" + -0.082*\"leisere\"\n"
     ]
    }
   ],
   "source": [
    "TOPIC_COUNT = 6\n",
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=TOPIC_COUNT)\n",
    "corpus_lsi = lsi[corpus_tfidf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Ergebnis ist eine Liste von Themen (ohne Namen, nur mit Index 0, 1, 2, ...) sowie der wichtigsten darin enthaltenen Wörter und Wortkombinationen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #0(4.002): -0.789*\"super\" + -0.459*\"essen\" + -0.214*\"kellner\" + -0.174*\"personal\" + -0.102*\"musik\" + -0.093*\"bedienung\" + -0.074*\"top\" + -0.073*\"echt\" + -0.064*\"qualität\" + -0.063*\"lecker\"\n",
      "INFO : topic #1(3.669): -0.836*\"essen\" + 0.439*\"super\" + 0.210*\"kellner\" + -0.087*\"top\" + -0.076*\"ambiente\" + -0.074*\"schmeckt\" + 0.072*\"musik\" + 0.055*\"personal\" + -0.051*\"tolle\" + -0.050*\"teuer\"\n",
      "INFO : topic #2(3.252): 0.904*\"kellner\" + -0.268*\"super\" + -0.168*\"personal\" + 0.155*\"blonde\" + 0.081*\"essen\" + 0.061*\"bedienung\" + -0.060*\"lokal\" + 0.054*\"netter\" + 0.053*\"könnten\" + 0.048*\"is\"\n",
      "INFO : topic #3(3.052): 0.732*\"personal\" + 0.512*\"lokal\" + -0.239*\"super\" + 0.173*\"musik\" + 0.114*\"getränke\" + 0.107*\"freundliches\" + 0.086*\"kellner\" + 0.079*\"angebot\" + 0.072*\"tisch\" + 0.062*\"bedienung\"\n",
      "INFO : topic #4(2.924): -0.802*\"musik\" + -0.241*\"top\" + -0.217*\"laut\" + -0.199*\"gerne\" + -0.190*\"perfekt\" + 0.172*\"personal\" + 0.147*\"lokal\" + 0.105*\"kellner\" + -0.102*\"unterhalten\" + -0.082*\"leisere\"\n",
      "INFO : topic #5(2.812): -0.617*\"top\" + 0.361*\"musik\" + -0.303*\"speisen\" + -0.259*\"gerne\" + -0.223*\"perfekt\" + -0.195*\"passt\" + -0.183*\"lecker\" + -0.162*\"qualität\" + 0.123*\"essen\" + 0.105*\"laut\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '-0.789*\"super\" + -0.459*\"essen\" + -0.214*\"kellner\" + -0.174*\"personal\" + -0.102*\"musik\" + -0.093*\"bedienung\" + -0.074*\"top\" + -0.073*\"echt\" + -0.064*\"qualität\" + -0.063*\"lecker\"'),\n",
       " (1,\n",
       "  '-0.836*\"essen\" + 0.439*\"super\" + 0.210*\"kellner\" + -0.087*\"top\" + -0.076*\"ambiente\" + -0.074*\"schmeckt\" + 0.072*\"musik\" + 0.055*\"personal\" + -0.051*\"tolle\" + -0.050*\"teuer\"'),\n",
       " (2,\n",
       "  '0.904*\"kellner\" + -0.268*\"super\" + -0.168*\"personal\" + 0.155*\"blonde\" + 0.081*\"essen\" + 0.061*\"bedienung\" + -0.060*\"lokal\" + 0.054*\"netter\" + 0.053*\"könnten\" + 0.048*\"is\"'),\n",
       " (3,\n",
       "  '0.732*\"personal\" + 0.512*\"lokal\" + -0.239*\"super\" + 0.173*\"musik\" + 0.114*\"getränke\" + 0.107*\"freundliches\" + 0.086*\"kellner\" + 0.079*\"angebot\" + 0.072*\"tisch\" + 0.062*\"bedienung\"'),\n",
       " (4,\n",
       "  '-0.802*\"musik\" + -0.241*\"top\" + -0.217*\"laut\" + -0.199*\"gerne\" + -0.190*\"perfekt\" + 0.172*\"personal\" + 0.147*\"lokal\" + 0.105*\"kellner\" + -0.102*\"unterhalten\" + -0.082*\"leisere\"'),\n",
       " (5,\n",
       "  '-0.617*\"top\" + 0.361*\"musik\" + -0.303*\"speisen\" + -0.259*\"gerne\" + -0.223*\"perfekt\" + -0.195*\"passt\" + -0.183*\"lecker\" + -0.162*\"qualität\" + 0.123*\"essen\" + 0.105*\"laut\"')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi.print_topics(TOPIC_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Interpretation\n",
    "\n",
    "Die Daten ergeben leider keine Eindeutigen Trends, da wiederholt ähnliche Begriffe die Basis für die unterschiedlichen Kategorien sind. Mit der Konstante `TOPIC_COUNT` lässt sich experimentieren. Der Wert 6 scheint hier am einfachsten interpretierbaren Ergebnisse zu liefern.\n",
    "\n",
    "Eine mögliche Interpretation ist:\n",
    "\n",
    "* Essen: topic 1\n",
    "* Service: topic 2, topic 3\n",
    "* Musik: topic 4\n",
    "\n",
    "Klar ist, dass eine solche Analyse nur eine Basis für eine Kategoriesystem sein kann. Insbesondere aufgrund der vergleichsweise geringen Datenanzahl ist mit einer gewissen Unvollständigkeit zu rechnen.\n",
    "\n",
    "Unabhängig davon bestätigt das Ergebnis die bereits intuitiv vermuteten Kategorien Essen und Service. Die Kategorie Musik erscheint etwas zu speziell, dürfte aber in einem breiteren Kontext wie Ambiente/Events/Rahmenprogramm durchaus sinnvoll sein."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
