{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to sentiment analysis\n",
    "## Thomas Aglassinger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Agenda\n",
    "\n",
    "* Some background \n",
    "* What is sentiment analysis\n",
    "* Text parts: sentences and tokens\n",
    "* Topics and ratings\n",
    "* Special topics (slang terms, typos, emojis, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Some background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# About me\n",
    "* Thomas Aglassinger\n",
    "* Software engineer\n",
    "  * ...and everythig related to it\n",
    "  * worked in various areas (hospitals, banking, ecommerce)\n",
    "* Master's degree in information processing science\n",
    "* Open source: https://github.com/roskakori\n",
    "* Co-organizer PyGRAZ user community: https://pygraz.org/\n",
    "* Homepage: http://www.roskakori.at/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why talk about sentiment analysis?\n",
    "* I needed a topic for a master's thesis\n",
    "* Former colleague who founded a startup: \"We have these unstructured German text data that we'd like to analyze\"\n",
    "* Quick glance\n",
    "  * People write books about natural language processing (NLP)\n",
    "  * We have Python (``import re``)\n",
    "  * There are packages to deal with NLP (spaCy, nltk, ...)\n",
    "* \"Good enough, let's get started!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## About TeLLers\n",
    "* Guests in inns / restaurants give feedback to innkeeper\n",
    "  * non public\n",
    "  * login optional\n",
    "* Mobile web application for guest (Angular)\n",
    "* Web application for owner (Django)\n",
    "* Homepage: https://tellers.co.at/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## TeLLers feedback\n",
    "* Structured feedback (yes/no, \"on a scale from 1 to 10\")\n",
    "  * simple to analyze\n",
    "* Unstructured text feedback\n",
    "  * difficult to summarize and compare over time\n",
    "  * \"What did you enjoy the must during your visit?\"\n",
    "  * \"How can we improve our products and service?\"\n",
    "  * \"Is there anything else you want to tell us?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example screenshot\n",
    "![TeLLers screenshot](images/screenshot_tellers_feedback.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Innkeeper wants rough answers for...\n",
    "* How do people feel about certain areas of my business?\n",
    "* What are the perceived strengths of my business (\"unique sales propositions\")\n",
    "* What do I need to improve?\n",
    "\n",
    "And he wants to...:\n",
    "* Get this at a glance.\n",
    "* If necessary drill down into specific feedbacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Related applications\n",
    "* Service ticket systems\n",
    "* Preprocessing of customer email\n",
    "* Product reviews\n",
    "* Vibe from social media and forums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is sentiment analysis?\n",
    "* „systematically identify, extract, quantify, and study affective states and subjective information“. Source:  https://en.wikipedia.org/wiki/Sentiment_analysis\n",
    "* Collects opinions from text written in natural language and stores them in a structured way\n",
    "* Different levels:\n",
    "  * Document\n",
    "  * Sentence (possibly multiple per document)\n",
    "  * Aspect (possibly multiple per sentence)\n",
    "  \n",
    "(Sometimes also called: sentiment detection, opinion mining etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Document level\n",
    "* Example: product review sites\n",
    "* Several sentences describe the opinion\n",
    "* Summarize document in one rating, e.g. 3 starts out of 5 or thumbs up/down\n",
    "* Not very useful if there is both good and bad\n",
    "  * You want to preserve the good parts (or improve even further)\n",
    "  * You want to fix the bad part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sentence level\n",
    "* Split the document in sentences.\n",
    "* Example: “The Schnitzel is too small for a hungry student”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What's a schnitzel?\n",
    "![Schnitzel](images/Breitenlesau_Krug_Braeu_Schnitzel.jpg)\n",
    "Image by User: Benreis at wikivoyage shared, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=22713889"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Aspect level\n",
    "* Multiple aspects in the same sentence\n",
    "* Example: \"The schnitzel tastes very well but it is too small.\"\n",
    "  * Schnitzel/taste = good\n",
    "  * Schnitzel/size = bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Definition: Opinion (deluxe edition)\n",
    "* Example: “The schnitzel is too small for a hungry student” (Hans Meier, 2018-04-28, 13:12 UTC)\n",
    "* Consists of:\n",
    "  * Target entity: schnitzel\n",
    "  * Aspect: size\n",
    "  * Sentiment: bad \n",
    "  * Opinion holder: Hans Meier\n",
    "  * Posting time: 2018-04-28, 13:12 UTC\n",
    "  * Reason: “too small”\n",
    "  * Qualifier: “for a hungry student” → might be fine for others\n",
    "* Reference: Bing Liu, “Sentiment Analysis”, Cambridge Press, 2015, p. 22ff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Definition: Opinion (simplified)\n",
    "* Example: “The Schnitzel is too small for a hungry student” (Hans Meier, 2018-04-28, 13:12 UTC)\n",
    "* Consists of:\n",
    "  * Topic: food\n",
    "  * Sentiment: bad \n",
    "  * Opinion holder: Hans Meier\n",
    "  * Posting time: 2018-04-28, 13:12 UTC\n",
    "* Enough to get a grip about \n",
    "  * Pain points\n",
    "  * Unique sales propositions (USPs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Basic workflow\n",
    "\n",
    "1. Collect data\n",
    "2. Preprocess data so the can be analyzed\n",
    "3. Analyze data\n",
    "4. Interpret and act on results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Basic workflow with TeLLers feedback\n",
    "\n",
    "1. Collect data → **already happened**\n",
    "2. Preprocess data so the can be analyzed → **a little towards the end of the presentation**\n",
    "3. Analyze data → **main focus of this presentation**\n",
    "4. Interpret and act on results → **done by innkeeper using analysis UI**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Enough of the pleasentries...\n",
    "![Brace yourself code incoming](images/brace_yourself_code_incoming.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text parts: sentences and tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Splitting a document in sentences and words\n",
    "This is easy, right?\n",
    "\n",
    "So here's a feedback document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_simple = 'The schnitzel tastes good. The soup was too hot. The waiter was quick and polite.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting it into senteces is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The schnitzel tastes good\n",
      " The soup was too hot\n",
      " The waiter was quick and polite\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = feedback_simple.split('.')\n",
    "print('\\n'.join(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Splitting words\n",
    "\n",
    "Now the same for words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "schnitzel\n",
      "tastes\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "words = sentences[0].split(' ')\n",
    "print('\\n'.join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thas was easy, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What about this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_rude = '''The waiter was very rude, \n",
    "e.g. when I accidentally opened the wrong door\n",
    "he screamed \"Private!\".'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try again with our trusty algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The waiter was very rude, \n",
      "e\n",
      "g\n",
      " when I accidentally opened the wrong door\n",
      "he screamed \"Private!\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = feedback_rude.split('.')\n",
    "print('\\n'.join(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## SpaCy to the rescue\n",
    "\n",
    "There are better ways to do this. Spacy (but also other packages like nltk) provide standard functions for this.\n",
    "\n",
    "First load the English language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next parse the document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The schnitzel tastes good.\n",
      "The soup was too hot.\n",
      "The waiter was quick and polite.\n"
     ]
    }
   ],
   "source": [
    "document = nlp_en(feedback_simple)\n",
    "for sentence in document.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Abbreviations and indirect speech galore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The waiter was very rude, \n",
      "e.g. when I accidentally opened the wrong door\n",
      "he screamed \"Private!\".\n"
     ]
    }
   ],
   "source": [
    "document_rude = nlp_en(feedback_rude)\n",
    "for sentence in document_rude.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Split into words\n",
    "\n",
    "Even though sentences print as simple strings, they are actually lists of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "schnitzel\n",
      "tastes\n",
      "good\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "first_sent = next(document.sents)\n",
    "for word in first_sent:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tokens\n",
    "\n",
    "Even though words are printed as simple strings, they actually are \"tokens\" and include meta information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tastes\n"
     ]
    }
   ],
   "source": [
    "tastes_token = first_sent[2]\n",
    "print(tastes_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'taste'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tastes_token.lemma_  # basic form of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VERB'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tastes_token.pos_  # \"part of speech\" = role of word in sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Token attributes\n",
    "\n",
    "Full list: https://spacy.io/api/token#attributes\n",
    "\n",
    "Many attributes have two variant with or without underscore at the end of the name, for example ``lemma`` and ``lemma_``. The first are integer codes that are compact to store and quick to compare while the latter are easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERB\n"
     ]
    }
   ],
   "source": [
    "print(tastes_token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tastes_token.pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Converting between spaCy names and IDs\n",
    "\n",
    "Import what you need from ``spacy.symbols``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "from spacy.symbols import ADJ, NOUN, VERB\n",
    "print(VERB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERB\n"
     ]
    }
   ],
   "source": [
    "from spacy.symbols import NAMES\n",
    "\n",
    "print(NAMES[99])  # 99 = VERB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "from spacy.symbols import IDS\n",
    "print(IDS['VERB'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Limitations of spaCy\n",
    "\n",
    "* Tokenizers use probabilistic models.\n",
    "* ``lemma`` and ``pos`` can sometimes be wrong.\n",
    "* Typically good enough.\n",
    "* If not: build you own model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topics and ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Topics\n",
    "\n",
    "There are several ways to find appropriate topics, for example:\n",
    "\n",
    "* Look what others use in similar situations\n",
    "* Automatic detection using topic modelling, for example ``gensim``: https://radimrehurek.com/gensim/\n",
    "* Build a tag cloud and see if it's useful\n",
    "* Ask domain experts (here: innkeepers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Actual topics for our example\n",
    "\n",
    "* **ambience**: decoration, space, light, music, temperature, ...\n",
    "* **food and beverages**: eating, drinking, taste, menu, selection\n",
    "* **hygiene**: toilett, smell, ...\n",
    "* **service**: waiters, reaction times, courtesy, competence, availability, ...\n",
    "* **value**: price, size of portions, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Topics as code\n",
    "\n",
    "This can be represented as ``Enum``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class Topic(Enum):\n",
    "    AMBIENCE = 1\n",
    "    FOOD = 2\n",
    "    HYGIENE = 3\n",
    "    SERVICE = 4\n",
    "    VALUE = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Rating (sentiment)\n",
    "\n",
    "There are serveral ways to represent a rating, for example:\n",
    "\n",
    "* Two distinct values \"prositive\" and \"negative\"\n",
    "* Same as above but with more distinct values, e.g. 1 to 5 stars\n",
    "* use a float between e.g. 0 and 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Rating as code\n",
    "\n",
    "For our example, we are going to use 3 degrees in both directions and represent them as an ``Enum``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rating(Enum):\n",
    "    VERY_BAD = -3\n",
    "    BAD = -2\n",
    "    SOMEWHAT_BAD = -1\n",
    "    SOMEWHAT_GOOD = 1\n",
    "    GOOD = 2\n",
    "    VERY_GOOD = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Contents of the lexicon\n",
    "* words relevant for our domain in their basic form (lemma)\n",
    "  * here: words can be regular expression\n",
    "  * for example: `.*schnitzel`\n",
    "  * accepts various kinds, for instance \"schnitzel\" and \"surschnitzel\"\n",
    "* possible topic\n",
    "* possible rating\n",
    "* can easily be stored in spreadsheet, data base etc\n",
    "\n",
    "Examples:\n",
    "\n",
    "```\n",
    "Lemma        Topic      Rating\n",
    "------------ ---------- ------\n",
    "waiter       service\n",
    "waitress     service\n",
    "wait                    bad\n",
    "quick                   good\n",
    ".*schnitzel  food\n",
    "music        ambience\n",
    "loud                    bad\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How to collect words for lexicon?\n",
    "\n",
    "* Add words that are obvious and easy to find, for example collect food term from menu\n",
    "* Find the most common words in raw data and examine them\n",
    "* Analyse data early version and check sentences with no topic or rating for interesting words --> iterative improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Lexicon entries in Python\n",
    "* mostly a data container\n",
    "* but we also want to be able to compare if it matches a spaCy `Token` --> we need a `matching()` function.\n",
    "* tokens can match exactly or only after transformations (for example upper/lower case) --> score between 0 (no match) and 1 (perfect match)\n",
    "\n",
    "\n",
    "And as we want to be able to use regular expressions and spaCy `Token` we need to import them now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from spacy.tokens import Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class LexiconEntry:\n",
    "    _IS_REGEX_REGEX = re.compile(r'.*[.+*\\[$^\\\\]')\n",
    "\n",
    "    def __init__(self, lemma: str, topic: Topic, rating: Rating):\n",
    "        assert lemma is not None\n",
    "        self.lemma = lemma\n",
    "        self._lower_lemma = lemma.lower()\n",
    "        self.topic = topic\n",
    "        self.rating = rating\n",
    "        self.is_regex = bool(LexiconEntry._IS_REGEX_REGEX.match(self.lemma))\n",
    "        self._regex = re.compile(lemma, re.IGNORECASE) if self.is_regex else None\n",
    "\n",
    "    def matching(self, token: Token) -> float:\n",
    "        \"\"\"\n",
    "        A weight between 0.0 and 1.0 on how much ``token`` matches this entry.\n",
    "        \"\"\"\n",
    "        assert token is not None\n",
    "        result = 0.0\n",
    "        if self.is_regex:\n",
    "            if self._regex.match(token.text):\n",
    "                result = 0.6\n",
    "            elif self._regex.match(token.lemma_):\n",
    "                result = 0.5\n",
    "        else:\n",
    "            if token.text == self.lemma:\n",
    "                result = 1.0\n",
    "            elif token.text.lower() == self.lemma:\n",
    "                result = 0.9\n",
    "            elif token.lemma_ == self.lemma:\n",
    "                result = 0.8\n",
    "            elif token.lemma_.lower() == self.lemma:\n",
    "                result = 0.7\n",
    "        return result\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        result = 'LexiconEntry(%s' % self.lemma\n",
    "        if self.topic is not None:\n",
    "            result += ', topic=%s' % self.topic.name\n",
    "        if self.rating is not None:\n",
    "            result += ', rating=%s' % self.rating.name\n",
    "        if self.is_regex:\n",
    "            result += ', is_regex=%s' % self.is_regex\n",
    "        result += ')'\n",
    "        return result\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.__str__()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The lexicon in Python\n",
    "* Contains a list of `LexiconEntry`\n",
    "* Can find the best matching entry for a `Token` (or `None`)\n",
    "* In the beginning entries have to be added\n",
    "* manually for our example, in practice from e.g .CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from math import isclose\n",
    "\n",
    "class Lexicon:\n",
    "    def __init__(self):\n",
    "        self.entries: List[LexiconEntry] = []\n",
    "\n",
    "    \n",
    "    def append(self, lemma: str, topic: Topic, rating: Rating):\n",
    "        lexicon_entry = LexiconEntry(lemma, topic, rating)\n",
    "        self.entries.append(lexicon_entry)\n",
    "\n",
    "    def lexicon_entry_for(self, token: Token) -> LexiconEntry:\n",
    "        \"\"\"\n",
    "        Entry in lexicon that best matches ``token``.\n",
    "        \"\"\"\n",
    "        result = None\n",
    "        lexicon_size = len(self.entries)\n",
    "        lexicon_entry_index = 0\n",
    "        best_matching = 0.0\n",
    "        while lexicon_entry_index < lexicon_size and not isclose(best_matching, 1.0):\n",
    "            lexicon_entry = self.entries[lexicon_entry_index]\n",
    "            matching = lexicon_entry.matching(token)\n",
    "            if matching > best_matching:\n",
    "                result = lexicon_entry\n",
    "                best_matching = matching\n",
    "            lexicon_entry_index += 1\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Let's build a small lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = Lexicon()\n",
    "lexicon.append('waiter'     , Topic.SERVICE , None)\n",
    "lexicon.append('waitress'   , Topic.SERVICE , None)\n",
    "lexicon.append('wait'       , None          , Rating.BAD)\n",
    "lexicon.append('quick'      , None          , Rating.GOOD)\n",
    "lexicon.append('.*schnitzel', Topic.FOOD    , None)\n",
    "lexicon.append('music'      , Topic.AMBIENCE, None)\n",
    "lexicon.append('loud'       , None          , Rating.BAD)\n",
    "lexicon.append('tasty'      , Topic.FOOD    , Rating.GOOD)\n",
    "lexicon.append('polite'     , Topic.SERVICE , Rating.GOOD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Matching tokens in a sentence to a lexicon entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The        None\n",
      "music      LexiconEntry(music, topic=AMBIENCE)\n",
      "was        None\n",
      "very       None\n",
      "loud       LexiconEntry(loud, rating=BAD)\n",
      ".          None\n"
     ]
    }
   ],
   "source": [
    "feedback_text = 'The music was very loud.'\n",
    "feedback = nlp_en(feedback_text)\n",
    "for token in next(feedback.sents):\n",
    "    lexicon_entry = lexicon.lexicon_entry_for(token)\n",
    "    print(f'{token!s:10} {lexicon_entry}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Yeah, our first simple sentiment analysis!\n",
    "\n",
    "Just add some filters and format the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The music was very loud.\n",
      "     Topic.AMBIENCE\n",
      "     Rating.BAD\n"
     ]
    }
   ],
   "source": [
    "feedback_text = 'The music was very loud.'\n",
    "feedback = nlp_en(feedback_text)\n",
    "for sent in feedback.sents:\n",
    "    print(sent)\n",
    "    for token in sent:\n",
    "        lexicon_entry = lexicon.lexicon_entry_for(token)\n",
    "        if lexicon_entry is not None:\n",
    "            if lexicon_entry.topic is not None:\n",
    "                print('    ', lexicon_entry.topic)\n",
    "            if lexicon_entry.rating is not None:\n",
    "                print('    ', lexicon_entry.rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The end? Not quite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Intensifiers, diminishers, negations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Intensifiers and diminishers\n",
    "\n",
    "* increase or decrease the rating of sentiment words\n",
    "* examples:\n",
    "  * diminishers: barely, slightly, somewhat, ...\n",
    "  * intensifiers: really, terribly, very, ...\n",
    "\n",
    "Impact on \"loud\":\n",
    "* \"loud\": `Rating.BAD`\n",
    "* \"very loud\": `Rating.VERY_BAD`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Intensifiers and diminishers in Python\n",
    "\n",
    "Use sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTENSIFIERS = {\n",
    "    'really',\n",
    "    'terribly',\n",
    "    'very',\n",
    "}\n",
    "\n",
    "def is_intensifier(token: Token) -> bool:\n",
    "    return token.lemma_.lower() in INTENSIFIERS\n",
    "\n",
    "DIMINISHERS = {\n",
    "    'barely',\n",
    "    'slightly',\n",
    "    'somewhat',\n",
    "}\n",
    "\n",
    "def is_diminisher(token: Token) -> bool:\n",
    "    return token.lemma_.lower() in DIMINISHERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Find out if a token is an intensifier\n",
    "\n",
    "For a little test, get the 4th token in 1st sentence, which is \"very\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "very\n"
     ]
    }
   ],
   "source": [
    "very_token = next(nlp_en(feedback_text).sents)[3]\n",
    "print(very_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_intensifier(very_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Intensify or diminish a Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating.SOMEWHAT_BAD\n",
      "Rating.SOMEWHAT_BAD\n",
      "Rating.VERY_BAD\n"
     ]
    }
   ],
   "source": [
    "def signum(value) -> int:\n",
    "    if value > 0:\n",
    "        return 1\n",
    "    elif value < 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "_MIN_RATING_VALUE = Rating.VERY_BAD.value\n",
    "_MAX_RATING_VALUE = Rating.VERY_GOOD.value\n",
    "\n",
    "\n",
    "def _ranged_rating(rating_value: int) -> Rating:\n",
    "    return Rating(min(_MAX_RATING_VALUE, max(_MIN_RATING_VALUE, rating_value)))\n",
    "\n",
    "def diminished(rating: Rating) -> Rating:\n",
    "    if abs(rating.value) > 1:\n",
    "        return _ranged_rating(rating.value - signum(rating.value))\n",
    "    else:\n",
    "        return rating\n",
    "\n",
    "def intensified(rating: Rating) -> Rating:\n",
    "    if abs(rating.value) > 1:\n",
    "        return _ranged_rating(rating.value + signum(rating.value))\n",
    "    else:\n",
    "        return rating\n",
    "\n",
    "print(diminished(Rating.BAD))\n",
    "print(diminished(Rating.SOMEWHAT_BAD))\n",
    "print(intensified(Rating.BAD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Negations\n",
    "\n",
    "* turn a sentiment to the opposit\n",
    "* example: \"not\"\n",
    "  * \"tasty\" = Rating.**GOOD**\n",
    "  * \"not tasty\" = Rating.**BAD**\n",
    "* can be combined with intensifiers and diminishers\n",
    "* example:\n",
    "  * \"very good\" = Rating.**VERY**_GOOD\n",
    "  * \"not very good\" = Rating.**SOMEWHAT**_BAD \n",
    "* negation also swaps intensifier and diminisher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Negations in Python\n",
    "Detection is similar to intensifiers and diminishers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEGATIONS = {\n",
    "    'no',\n",
    "    'not',\n",
    "    'none',\n",
    "}\n",
    "\n",
    "def is_negation(token: Token) -> bool:\n",
    "    return token.lemma_.lower() in NEGATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Negation of a Rating\n",
    "Negating a `Rating` is classic mapping issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating.GOOD  ->  Rating.BAD\n",
      "Rating.VERY_BAD  ->  Rating.SOMEWHAT_GOOD\n"
     ]
    }
   ],
   "source": [
    "_RATING_TO_NEGATED_RATING_MAP = {\n",
    "    Rating.VERY_BAD     : Rating.SOMEWHAT_GOOD,\n",
    "    Rating.BAD          : Rating.GOOD,\n",
    "    Rating.SOMEWHAT_BAD : Rating.GOOD,  # hypothetical?\n",
    "    Rating.SOMEWHAT_GOOD: Rating.BAD,  # hypothetical?\n",
    "    Rating.GOOD         : Rating.BAD,\n",
    "    Rating.VERY_GOOD    : Rating.SOMEWHAT_BAD,\n",
    "}\n",
    "\n",
    "def negated_rating(rating: Rating) -> Rating:\n",
    "    assert rating is not None\n",
    "    return _RATING_TO_NEGATED_RATING_MAP[rating]\n",
    "\n",
    "print(Rating.GOOD, ' -> ', negated_rating(Rating.GOOD))\n",
    "print(Rating.VERY_BAD, ' -> ', negated_rating(Rating.VERY_BAD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## So far so good\n",
    "Based on a simple lexicon and a few Python sets we can now assign sentiment information to single tokens concerning:\n",
    "\n",
    "* Topic\n",
    "* Rating\n",
    "* intensifiers / diminishers\n",
    "* nagations\n",
    "\n",
    "However, we still need to combine multiple tokens in our analysis. We could of course start messing with lists of tokens. However, spaCy offers nice possibilities for such situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Extending spaCy's pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What's this pipleline thingy?\n",
    "\n",
    "* When you pass a text to spaCy's `nlp()` it performs multiple separate steps until it ends up with tokens and all their attributes\n",
    "* Nice and clean \"separation of concerns\" (basic software principle)\n",
    "* `Token` can get additional attributes (same goes for documents (`Doc`) and sents (`Span`), but we don't need this right now)\n",
    "* Steps can be added or removed from the pipeline\n",
    "\n",
    "Recommended reading: https://explosion.ai/blog/spacy-v2-pipelines-extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Extending Token\n",
    "We can add new attributes for sentiment relevant information to the extensible \"underscore\" attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Token.set_extension('topic', default=None)\n",
    "Token.set_extension('rating', default=None)\n",
    "Token.set_extension('is_negation', default=False)\n",
    "Token.set_extension('is_intensifier', default=False)\n",
    "Token.set_extension('is_diminisher', default=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can set and examine these attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schnitzel\n",
      "Topic.FOOD\n"
     ]
    }
   ],
   "source": [
    "token = next(nlp_en('schnitzel').sents)[0]\n",
    "print(token.lemma_)\n",
    "token._.topic = Topic.FOOD\n",
    "print(token._.topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Intermission: a small debugging function\n",
    "In order to print tokens including the new attributes here's a little helper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token(schnitzel, lemma=schnitzel, topic=FOOD)\n"
     ]
    }
   ],
   "source": [
    "def debugged_token(token: Token) -> str:\n",
    "    result = 'Token(%s, lemma=%s' % (token.text, token.lemma_)\n",
    "    if token._.topic is not None:\n",
    "        result += ', topic=' + token._.topic.name\n",
    "    if token._.rating is not None:\n",
    "        result += ', rating=' + token._.rating.name\n",
    "    if token._.is_diminisher:\n",
    "        result += ', diminisher'\n",
    "    if token._.is_intensifier:\n",
    "        result += ', intensifier'\n",
    "    if token._.is_negation:\n",
    "        result += ', negation'\n",
    "    result += ')'\n",
    "    return result\n",
    "\n",
    "print(debugged_token(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Extending the pipeline\n",
    "First we need a function to add to the pipeline that sets our new `Token` attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opinion_matcher(doc):\n",
    "    for sentence in doc.sents:\n",
    "        for token in sentence:\n",
    "            if is_intensifier(token):\n",
    "                token._.is_intensifier = True\n",
    "            elif is_diminisher(token):\n",
    "                token._.is_diminisher = True\n",
    "            elif is_negation(token):\n",
    "                token._.is_negation = True\n",
    "            else:\n",
    "                lexicon_entry = lexicon.lexicon_entry_for(token)\n",
    "                if lexicon_entry is not None:\n",
    "                    token._.rating = lexicon_entry.rating\n",
    "                    token._.topic = lexicon_entry.topic\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can actually add it to the pipeline (and remove it first if it already was part of the pipeline):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if nlp_en.has_pipe('opinion_matcher'):\n",
    "    nlp_en.remove_pipe('opinion_matcher')\n",
    "nlp_en.add_pipe(opinion_matcher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Extracting token relevant for the opinion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the information attached to the token it is simple to reduce a sentence to its essential information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_essential(token: Token) -> bool:\n",
    "    return token._.topic is not None \\\n",
    "        or token._.rating is not None \\\n",
    "        or token._.is_diminisher \\\n",
    "        or token._.is_intensifier \\\n",
    "        or token._.is_negation\n",
    "        \n",
    "def essential_tokens(tokens):\n",
    "    return [token for token in tokens if is_essential(token)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token(schnitzel, lemma=schnitzel, topic=FOOD)\n",
      "Token(not, lemma=not, negation)\n",
      "Token(very, lemma=very, intensifier)\n",
      "Token(tasty, lemma=tasty, topic=FOOD, rating=GOOD)\n"
     ]
    }
   ],
   "source": [
    "document = nlp_en('The schnitzel is not very tasty.')\n",
    "\n",
    "opinion_essence = essential_tokens(document)\n",
    "for token in opinion_essence:\n",
    "    print(debugged_token(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Apply on Rating\n",
    "\n",
    "Now we have all the building blocks to apply intensifiers, diminishers and nagations on the rating. The basic idea is that when we encounter a taken with a rating and modifiers to the left of it that we can combine them until we don't find any more.\n",
    "\n",
    "To keep things tidy, here's a another little helper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_rating_modifier(token: Token):\n",
    "    return token._.is_diminisher \\\n",
    "        or token._.is_intensifier \\\n",
    "        or token._.is_negation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example\n",
    "\n",
    "The previous sentence yielded the tokens:\n",
    "```\n",
    "Token(schnitzel, lemma=schnitzel, topic=FOOD)\n",
    "Token(not, lemma=not, negation)\n",
    "Token(very, lemma=very, intensifier)\n",
    "Token(tasty, lemma=tasty, topic=FOOD, rating=GOOD)\n",
    "```\n",
    "\n",
    "We want to perform the following steps:\n",
    "1. Find the first rating from the left -> `tasty(GOOD)`\n",
    "2. Check if the token to the left is a modifer -> yes: `very(intensifier)`\n",
    "3. Combine them -> `(very) tasty(VERY_GOOD)` and remove the left token\n",
    "4. Check if the token to the left is a modifer -> yes: `not(negation)`\n",
    "5. Combine them -> `(not very) tasty(BAD)` and remove the left token\n",
    "6. End result: `not very tasty(GOOD)` -> `tasty(BAD)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def combine_ratings(tokens):\n",
    "    # Find the first rating (if any).\n",
    "    rating_token_index = next(\n",
    "        (\n",
    "            token_index for token_index in range(len(tokens))\n",
    "            if tokens[token_index]._.rating is not None\n",
    "        ),\n",
    "        None  # Default if no rating token can be found\n",
    "        \n",
    "    )\n",
    "\n",
    "    if rating_token_index is not None:\n",
    "        # Apply modifiers to the left on the rating.\n",
    "        original_rating_token = tokens[rating_token_index]\n",
    "        combined_rating = original_rating_token._.rating\n",
    "        modifier_token_index = rating_token_index - 1\n",
    "        modified = True  # Did the last iteration modify anything?\n",
    "        while modified and modifier_token_index >= 0:\n",
    "            modifier_token = tokens[modifier_token_index]\n",
    "            if is_intensifier(modifier_token):\n",
    "                combined_rating = intensified(combined_rating)\n",
    "            elif is_diminisher(modifier_token):\n",
    "                combined_rating = diminished(combined_rating)\n",
    "            elif is_negation(modifier_token):\n",
    "                combined_rating = negated_rating(combined_rating)\n",
    "            else:\n",
    "                # We are done, no more modifiers \n",
    "                # to the left of this rating.\n",
    "                modified = False\n",
    "            if modified:\n",
    "                # Discord the current modifier \n",
    "                # and move on to the token on the left.\n",
    "                del tokens[modifier_token_index]\n",
    "                modifier_token_index -= 1\n",
    "        original_rating_token._.rating = combined_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example for a combined rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "essential tokens:\n",
      "   Token(schnitzel, lemma=schnitzel, topic=FOOD)\n",
      "   Token(not, lemma=not, negation)\n",
      "   Token(very, lemma=very, intensifier)\n",
      "   Token(tasty, lemma=tasty, topic=FOOD, rating=GOOD)\n",
      "combined tokens:\n",
      "   Token(schnitzel, lemma=schnitzel, topic=FOOD)\n",
      "   Token(tasty, lemma=tasty, topic=FOOD, rating=SOMEWHAT_BAD)\n"
     ]
    }
   ],
   "source": [
    "document = nlp_en('The schnitzel is not very tasty.')\n",
    "\n",
    "opinion_essence = essential_tokens(document)\n",
    "print('essential tokens:')\n",
    "for token in opinion_essence:\n",
    "    print('  ', debugged_token(token))\n",
    "\n",
    "combine_ratings(opinion_essence)\n",
    "print('combined tokens:')\n",
    "for token in opinion_essence:\n",
    "    print('  ', debugged_token(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A function to extract topic and rating of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The schnitzel is not very tasty.\n",
      "(<Topic.FOOD: 2>, <Rating.SOMEWHAT_BAD: -1>)\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple  # for fancy type hints\n",
    "\n",
    "def topic_and_rating_of(tokens: List[Token]) -> Tuple[Topic, Rating]:\n",
    "    result_topic = None\n",
    "    result_rating = None\n",
    "    opinion_essence = essential_tokens(tokens)\n",
    "    # print('  1: ', opinion_essence)\n",
    "    combine_ratings(opinion_essence)\n",
    "    # print('  2: ', opinion_essence)\n",
    "    for token in opinion_essence:\n",
    "        # print(debugged_token(token))\n",
    "        if (token._.topic is not None) and (result_topic is None):\n",
    "            result_topic = token._.topic\n",
    "        if (token._.rating is not None) and (result_rating is None):\n",
    "            result_rating = token._.rating\n",
    "        if (result_topic is not None) and (result_rating is not None):\n",
    "            break\n",
    "    return result_topic, result_rating\n",
    "\n",
    "sentence = next(nlp_en('The schnitzel is not very tasty.').sents)\n",
    "\n",
    "print(sentence)\n",
    "print(topic_and_rating_of(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A function to extract opinions from feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opinions(feedback_text: str):\n",
    "    feedback = nlp_en(feedback_text)\n",
    "    for tokens in feedback.sents:\n",
    "        yield(topic_and_rating_of(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic.FOOD Rating.SOMEWHAT_BAD\n",
      "Topic.SERVICE Rating.GOOD\n",
      "None None\n"
     ]
    }
   ],
   "source": [
    "feedback_text = \"\"\"\n",
    "The schnitzel was not very tasty. \n",
    "The waiter was polite.\n",
    "The football game ended 2:1.\"\"\"\n",
    "\n",
    "for topic, rating in opinions(feedback_text):\n",
    "    print(topic, rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary:\n",
    "* Food needs improvement, Service is fine.\n",
    "* Football results are of no interest to us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Enough with the code!\n",
    "![mind blown](images/mind_blown.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What's next?\n",
    "\n",
    "Plenty!\n",
    "\n",
    "* modals: \"could\", \"should\" -> typically indicate a negative rating\n",
    "* idioms that indicate rating, for example \"Leaves a lot to be desired\"\n",
    "* back references: \"he\" (the waiter), \"it\" (the schnitzel) -> use topic from previous sentence if no other topic is given\n",
    "* add a topic hierarchy, for example `tasty` is about `food.taste`\n",
    "* Instead of careful handmade functions like `combine_rating()` use abstract rules that can be processed by a rule engine\n",
    "* ...and all the other linguist jazz!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Neverthess: the simple algorithm presented gets > 80% of feedback for inkeepers right. Which is good enough for its indended purpose of finding areas of interest that need further examination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Special topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Emojis\n",
    "* unify unicode emojis, western smileys and eastern smileys\n",
    "* assign rating to the them\n",
    "* Emojis in spaCy: https://github.com/ines/spacymoji\n",
    "* List of unicode emojis with suggested rating: http://www.unicode.org/emoji/charts/full-emoji-list.html (You still need to adjust it for your needs.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Slang terms\n",
    "* German vs Austrian\n",
    "* English vs Scottish\n",
    "* Preprocess and replace using synonyms.\n",
    "* Only for sentiment relevant terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUSTRIAN_TO_GERMAN_SYONYM_MAP = {\n",
    "    'nix': 'nichts',   # nothing\n",
    "    'ois': 'alles',    # everything\n",
    "    'kana': 'keiner',  # no one (more eastern parts)\n",
    "    'koana': 'keiner', # no one (more western parts)\n",
    "    # ...\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Unknown abbreviations\n",
    "* example for know abbreviation: \"resp.\" (respectively)\n",
    "* breaks sentence detection\n",
    "* if common: contribute to spaCy: module `tokenizer_exceptions`\n",
    "* if uncommon: add synonym for expanded form as preprocessing step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Typos\n",
    "* if word is not relevant for topics and ratings: ignore\n",
    "* common typos: use synonyms (similar to slang terms)\n",
    "* uncommon typos:\n",
    "  * Is this really a thing?\n",
    "  * Mobile input correction to the rescue\n",
    "  * If you must:\n",
    "    * fuzzy search in lexicon, for example: https://github.com/seatgeek/fuzzywuzzy\n",
    "    * spelling dictionary (independent of lexicon)\n",
    "  * Not needed for TeLLers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References\n",
    "* Liu (2015) - Bing Liu. Sentiment Analysis. Cambridge, MA: Cambridge University Press, 2015.\n",
    "* Montani (2017) - Ines Montani. Introducing custom pipelines and extensions for spaCy v2.0. https://explosion.ai/blog/spacy-v2-pipelines-extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "* Sentiment analysis is challenging.\n",
    "* Python and spaCy help a lot with the development part.\n",
    "* Code complexity can remain managable."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
