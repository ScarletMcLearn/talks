{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment detection with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations\n",
    "Various settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "tellers_db_path = '/tmp/tellers.db'\n",
    "lexicon_csv_path = 'lexicon_de.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import various modules we are going to need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python standard library\n",
    "import csv\n",
    "import re\n",
    "import sqlite3\n",
    "from contextlib import closing\n",
    "from enum import Enum\n",
    "\n",
    "# SpaCy\n",
    "import spacy\n",
    "from spacy.tokens import Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read feedbacks from database\n",
    "\n",
    "Connect to the database with feedback texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = sqlite3.connect(tellers_db_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class to store a feedback and the related question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedback():\n",
    "    def __init__(self, question_id: int, question: str, feedback_id: int, text: str):\n",
    "        self.question_id = question_id\n",
    "        self.question = question\n",
    "        self.feedback_id = feedback_id\n",
    "        self.text = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the feedback documents, where each feedback can consist of multiple sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 298 feedback documents\n"
     ]
    }
   ],
   "source": [
    "select_feedback_sql = \"\"\"\n",
    "    select\n",
    "        que.question_id,\n",
    "        que.text,\n",
    "        fdb.feedback_id,\n",
    "        fdb.text\n",
    "    from\n",
    "        feedback as fdb\n",
    "        join source as src on\n",
    "            src.source_id = fdb.source_id\n",
    "        join question as que on\n",
    "            que.question_id = fdb.question_id\n",
    "    where 1 = 1\n",
    "        and fdb.feedback_time >= '2017-10-01'\n",
    "        and src.name = 'tellers'\n",
    "\"\"\"\n",
    "\n",
    "with closing(connection.cursor()) as cursor:\n",
    "    feedbacks = [\n",
    "        Feedback(question_id, question,feedback_id, feedback)\n",
    "        for question_id, question, feedback_id, feedback in cursor.execute(select_feedback_sql)\n",
    "    ]\n",
    "print('found {} feedback documents'.format(len(feedbacks)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup texts for further processing\n",
    "Replace certain abbrevisations that would confuse spaCy when detection sentence borders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replaced 1 abbreviations\n"
     ]
    }
   ],
   "source": [
    "ABBREVIATION_TO_EXPANDED_MAP = {\n",
    "    'ca': 'circa',  # \"approximately\"\n",
    "    'ev': 'eventuell',  # \"possibly\"\n",
    "    'max': 'maximal',\n",
    "    'vlt': 'vielleicht',  # \"maybe\"\n",
    "}\n",
    "\n",
    "replace_count = 0\n",
    "for feedback in feedbacks:\n",
    "    for abbreviation, expanded in ABBREVIATION_TO_EXPANDED_MAP.items():\n",
    "        # TODO: Use compiled regex.\n",
    "        previous_text = feedback.text\n",
    "        feedback.text = re.sub(\n",
    "            r'\\b' + abbreviation + r'\\.', \n",
    "            expanded + ' ',\n",
    "            feedback.text,\n",
    "            flags=re.IGNORECASE)\n",
    "        if feedback.text != previous_text:\n",
    "            replace_count += 1\n",
    "print('replaced %d abbreviations' % replace_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a map of emojis (both western and eastern) to a distinct text form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_EMOJI_PREFIX = 'emoji__'\n",
    "_EMOJI_TO_NAME_MAP = {\n",
    "    # Western\n",
    "    ':)': 'slight_smile',\n",
    "    ':-)': 'slight_smile',\n",
    "    '=)': 'slight_smile',\n",
    "    ':(': 'slight_frown',\n",
    "    ':-(': 'slight_frown',\n",
    "    ':D': 'smile',\n",
    "    ':-D': 'smile',\n",
    "    ':P': 'stuck_out_tongue',\n",
    "    ':-P': 'stuck_out_tongue',\n",
    "    ';)': 'wink',\n",
    "    ';-)': 'wink',\n",
    "    # Eastern\n",
    "    '^^': 'slight_smile',\n",
    "    '^_^': 'slight_smile',\n",
    "}\n",
    "_EMOJI_TO_TEXT_MAP = {\n",
    "    emoji: ' ' + _EMOJI_PREFIX + name + ' '\n",
    "    for emoji, name in _EMOJI_TO_NAME_MAP.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace emojis by text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feedback in feedbacks:\n",
    "    for emoji, emoji_text in _EMOJI_TO_TEXT_MAP.items():\n",
    "        feedback.text = feedback.text.replace(emoji, emoji_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace some Austrian slang term by proper German:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replaced 5 austrian slang terms\n"
     ]
    }
   ],
   "source": [
    "_AUSTRIAN_TO_GERMAN_SYNONYM_MAP = {\n",
    "    'eh': 'ohnehin',\n",
    "    'nix': 'nichts',\n",
    "    'ois': 'alles',\n",
    "}\n",
    "\n",
    "replace_count = 0\n",
    "for feedback in feedbacks:\n",
    "    for austrian_word, german_word in _AUSTRIAN_TO_GERMAN_SYNONYM_MAP.items():\n",
    "        # TODO: Use compiled regex.\n",
    "        previous_text = feedback.text\n",
    "        feedback.text = re.sub(\n",
    "            r'\\b' + austrian_word + r'\\b', \n",
    "            german_word + ' ',\n",
    "            feedback.text,\n",
    "            flags=re.IGNORECASE)\n",
    "        if feedback.text != previous_text:\n",
    "            replace_count += 1\n",
    "print('replaced %d austrian slang terms' % replace_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into sentences\n",
    "Definde a class to hold a single opinion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Opinion():\n",
    "    def __init__(self, feedback: Feedback, sentence_nr: int, tokens):\n",
    "        self.feedback = feedback\n",
    "        self.sentence_nr = sentence_nr\n",
    "        self.tokens = list(tokens)\n",
    "        self.topic = None\n",
    "        self.rating = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the feedbacks into sentences and assign the sentence to an opinion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 481 opinions\n"
     ]
    }
   ],
   "source": [
    "opinions = []\n",
    "for feedback in feedbacks:\n",
    "    document = nlp(feedback.text)\n",
    "    sentence_nr = 1\n",
    "    for sentence in document.sents:\n",
    "        opinion = Opinion(feedback, sentence_nr, sentence)\n",
    "        opinions.append(opinion)\n",
    "        sentence_nr += 1\n",
    "print('found', len(opinions), 'opinions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics\n",
    "\n",
    "There are several ways to find appropriate topics. We are going to use:\n",
    "\n",
    "* ambience: decoration, space, light, temperature, ...\n",
    "* food and beverages: eating, drinking, taste, menu, selection\n",
    "* hygiene: toilett, smell, ...\n",
    "* service: waiting and reaction times, courtesy, competence, availability, ...\n",
    "* value: size of portions, price, ...\n",
    "\n",
    "This can be reprsented as Python Enum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Topic(Enum):\n",
    "    UNKNWON = -99\n",
    "    AMBIENCE = 1\n",
    "    FOOD = 2\n",
    "    HYGIENE = 3\n",
    "    SERVICE = 4\n",
    "    VALUE = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rating\n",
    "\n",
    "There are serverl ways to represent a rating:\n",
    "\n",
    "* Use \"prositive\" and \"negative\"\n",
    "* Same as above but with more distincz values, e.g. 1 to 5 stars\n",
    "* use a float between e.g. 0 and 1.0\n",
    "\n",
    "We are going to use a system with 6 discret values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rating(Enum):\n",
    "    UNKNOWN = -99\n",
    "    VERY_BAD = -4\n",
    "    BAD = -2\n",
    "    SOMEWHAT_BAD = -1\n",
    "    SOMEWHAT_GOOD = 1\n",
    "    GOOD = 2\n",
    "    VERY_GOOD = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicon\n",
    "\n",
    "In a lexicon based sentiment analysis, a lexicon collects words and assigns them to topics and ratings. It also includes more information a parser can utilize to combine multiple words.\n",
    "\n",
    "Different types of word have various implications on how an opinion can be extracted. A simple system that works with simple sentences:\n",
    "\n",
    "* noun: Schnitzel, Bier (beer), Geruch (smell)\n",
    "* adjective: toll (great), entäuschend (disappointing), wohlschmeckend (tasty)\n",
    "* verb: warten (to wait), stinken (to smell)\n",
    "* modifiers (intensify or dimish adjective): eher (somewhat), besonders (very), zu (too), viel zu (much to)\n",
    "* negator: nicht (not), kein\n",
    "\n",
    "Negators can also be prefixes like \"un\" and \"in\", e.g. brauchbar (suitable) - unbrauchbar (unfit) or kompetent (competent) - inkompetent (incompetent).\n",
    "\n",
    "Be aware that negators do not simply change the sign of a rating, for example:\n",
    "* \"schlecht\" (bad) - BAD\n",
    "* \"nicht schlecht\" (not bad) - SOMEWHAT_GOOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordType(Enum):\n",
    "    UNKNOWN = -99\n",
    "    NOUN = 1\n",
    "    ADJECTIVE = 2\n",
    "    VERB = 3\n",
    "    MODIFIER = 4\n",
    "    NEGATOR = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words can also be regular expressions to reduce the size. For example, various types of wine (\"Rotwein\", \"Weißwein\", \"Portwein\") can be reduced to the regular expression `r'.*wein'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lexicon entry combines all this information for each word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LexiconEntry():\n",
    "    _IS_REGEX_REGEX = re.compile(r'.*[.+*\\[$^\\\\]')\n",
    "    \n",
    "    def __init__(self, lemma: str, word_type: WordType, topic: Topic, rating: Rating):\n",
    "        assert rating is not None if word_type is WordType.MODIFIER else True, 'modifier must have rating: ' + lemma\n",
    "        self.lemma = lemma\n",
    "        self._lower_lemma = lemma.lower()\n",
    "        self.word_type = word_type\n",
    "        self.topic = topic\n",
    "        self.rating = rating\n",
    "        self.is_regex = LexiconEntry._IS_REGEX_REGEX.match(self.lemma) is not None       \n",
    "        self._regex = re.compile(lemma) if self.is_regex else None\n",
    "    \n",
    "    def matching(self, token: Token) -> float:\n",
    "        result = 0.0\n",
    "        if self.is_regex:\n",
    "            if self._regex.match(token.text):\n",
    "                result = 0.6\n",
    "            elif self._regex.match(token.lemma_):\n",
    "                result = 0.5\n",
    "        else:\n",
    "            if token.text == self.lemma:\n",
    "                result = 1.0\n",
    "            elif token.text.lower() == self.lemma:\n",
    "                result = 0.9\n",
    "            elif token.lemma_ == self.lemma:\n",
    "                result = 0.8\n",
    "            elif token.lemma_.lower() == self.lemma:\n",
    "                result = 0.7\n",
    "        return result\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        result = 'LexiconEntry(%s, word_type=%s' % (self.lemma, self.word_type.name)\n",
    "        if self.topic is not None:\n",
    "            result += ', topic=%s' % self.topic.name\n",
    "        if self.rating is not None:\n",
    "            result += ', rating=%s' % self.rating.name\n",
    "        if self.is_regex:\n",
    "            result += ', is_regex=%s' % self.is_regex\n",
    "        result += ')'\n",
    "        return result\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on this approach, see Liu (2015, p. 59ff)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing and reading the lexicon\n",
    "\n",
    "A simple way to store the lexicon is a CSV file with columns for:\n",
    "\n",
    "* lemma or pattern\n",
    "* word type\n",
    "* topic\n",
    "* rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 470 lexicon entries\n"
     ]
    }
   ],
   "source": [
    "_RATING_NAME_TO_VALUE_MAP = {some.name.lower(): some for some in Rating}\n",
    "_TOPIC_NAME_TO_VALUE_MAP = {some.name.lower(): some for some in Topic}\n",
    "_WORD_TYPE_NAME_TO_VALUE_MAP = {some.name.lower(): some for some in WordType}\n",
    "\n",
    "lexicon = []\n",
    "with open(lexicon_csv_path, encoding='utf-8', newline='') as lexicon_file:\n",
    "    lexicon_reader = csv.reader(lexicon_file, delimiter=',')\n",
    "    for row in lexicon_reader:\n",
    "        row = [item.strip() for item in row]\n",
    "        row += 4 * ['']  # Ensure we have at least 4 strings\n",
    "        lemma, word_type_text, topic_text, rating_text = row[:4]\n",
    "        if lemma != '' and not lemma.startswith('#'):\n",
    "            try:\n",
    "                # Map certain columns to enums.\n",
    "                word_type = _WORD_TYPE_NAME_TO_VALUE_MAP[word_type_text]\n",
    "                topic = _TOPIC_NAME_TO_VALUE_MAP.get(topic_text)\n",
    "                rating = _RATING_NAME_TO_VALUE_MAP.get(rating_text)\n",
    "            except KeyError as error:\n",
    "                raise csv.Error(\n",
    "                    '%s:%d: cannot map value: %s' % (\n",
    "                        lexicon_csv_path, lexicon_reader.line_num, error))\n",
    "            lexicon_entry = LexiconEntry(lemma, word_type, topic, rating)\n",
    "            lexicon.append(lexicon_entry)\n",
    "print('found %d lexicon entries' % len(lexicon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find base word (lemma) for token\n",
    "\n",
    "This can again be done with the help of SpaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Welche Änderungen müssten an unserem Restaurant vorgenommen werden, damit Sie eine bessere Bewertung abgeben?\n",
      "[Ambiente, eventuell, da, es, zurzeit, nicht, sehr, asiatisch, wirkt]\n",
      "Ambiente -> Ambiente\n",
      "eventuell -> eventuell\n",
      "da -> da\n",
      "es -> ich\n",
      "zurzeit -> zurzeit\n",
      "nicht -> nicht\n",
      "sehr -> sehr\n",
      "asiatisch -> asiatisch\n",
      "wirkt -> wirken\n",
      "\n",
      "Welche Änderungen müssten an unserem Restaurant vorgenommen werden, damit Sie eine bessere Bewertung abgeben?\n",
      "[Keine, alles, ist, tiptop, in, Ordnung, .,  ]\n",
      "Keine -> Keine\n",
      "alles -> alle\n",
      "ist -> sein\n",
      "tiptop -> tiptop\n",
      "in -> in\n",
      "Ordnung -> Ordnung\n",
      ". -> .\n",
      "  ->  \n",
      "\n",
      "Welche Änderungen müssten an unserem Restaurant vorgenommen werden, damit Sie eine bessere Bewertung abgeben?\n",
      "[emoji__slight_smile]\n",
      "emoji__slight_smile -> emoji__slight_smile\n"
     ]
    }
   ],
   "source": [
    "for opinion in opinions[:3]:\n",
    "    print()\n",
    "    print(opinion.feedback.question)\n",
    "    print(opinion.tokens)\n",
    "    for token in opinion.tokens:\n",
    "        print('%s -> %s' % (token.text, token.lemma_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match lemma with lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "LexiconEntry(lecker, word_type=ADJECTIVE, topic=FOOD, rating=GOOD)\n",
      "lecker\n",
      "lecker\n",
      "lecker\n",
      "LexiconEntry(lecker, word_type=ADJECTIVE, topic=FOOD, rating=GOOD)\n",
      "Ambiente -> Ambiente -> Topic.AMBIENCE, None\n",
      "eventuell -> eventuell -> None\n",
      "da -> da -> None\n",
      "es -> ich -> None\n",
      "zurzeit -> zurzeit -> None\n",
      "nicht -> nicht -> None, None\n",
      "sehr -> sehr -> None\n",
      "asiatisch -> asiatisch -> None\n",
      "wirkt -> wirken -> None\n"
     ]
    }
   ],
   "source": [
    "def lexicon_entry_for(token) -> LexiconEntry:\n",
    "    result = None\n",
    "    lemma = token.lemma_\n",
    "    lower_lemma = lemma.lower()\n",
    "    lexicon_entry_index = 0\n",
    "    best_matching = 0.0\n",
    "    # TODO: Improve performance by not having to scan whole lexicon for each token.\n",
    "    for lexicon_entry in lexicon:\n",
    "        matching = lexicon_entry.matching(token)\n",
    "        if matching > best_matching:\n",
    "            result = lexicon_entry\n",
    "            best_matching = matching\n",
    "    return result\n",
    "\n",
    "token = next(nlp('lecker').sents)\n",
    "lecker_entry = LexiconEntry('lecker', WordType.ADJECTIVE, Topic.FOOD, Rating.GOOD)\n",
    "print(lecker_entry.matching(token))\n",
    "print(lecker_entry)\n",
    "print(token)\n",
    "print(token.text)\n",
    "print(token.lemma_)\n",
    "print(lexicon_entry_for(token))\n",
    "\n",
    "for token in opinions[0].tokens:\n",
    "    matching_lexicon_entry = lexicon_entry_for(token)\n",
    "    if matching_lexicon_entry is None:\n",
    "        print('%s -> %s -> %s' % (token.text, token.lemma_, matching_lexicon_entry))\n",
    "    else:\n",
    "        print('%s -> %s -> %s, %s' % (\n",
    "            token.text, token.lemma_, matching_lexicon_entry.topic, matching_lexicon_entry.rating))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we cann reduce opinion senteces to lists of topics and ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LexiconEntry(Bratwurst, word_type=NOUN, topic=FOOD), LexiconEntry(lecker, word_type=ADJECTIVE, topic=FOOD, rating=GOOD)]\n"
     ]
    }
   ],
   "source": [
    "opinion_essence = []\n",
    "for token in nlp('Die Bratwurst schmeckt sehr lecker!'):\n",
    "    matching_lexicon_entry = lexicon_entry_for(token)\n",
    "    if matching_lexicon_entry is not None:\n",
    "        opinion_essence.append(matching_lexicon_entry)\n",
    "if len(opinion_essence) >= 1:\n",
    "    print(opinion_essence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add spaCy extensions for topic, rating, etc\n",
    "\n",
    "SpaCy has an extension API to store (among other things) additional attributes on documents, spans and tokens (see Montani (2017)). To add attributes for topic and rating we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Token.set_extension('topic', default=None)\n",
    "Token.set_extension('rating', default=None)\n",
    "Token.set_extension('is_negator', default=False)\n",
    "Token.set_extension('is_intensifier', default=False)\n",
    "Token.set_extension('is_dimisher', default=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now set and get these attributes using for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bratwurst\n",
      "Topic.FOOD\n"
     ]
    }
   ],
   "source": [
    "token = next(nlp('Bratwurst').sents)[0]\n",
    "print(token.lemma_)\n",
    "token._.topic = Topic.FOOD\n",
    "print(token._.topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify debugging the following function shows the Token and its relevant attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token(Bratwurst, lemma=Bratwurst, topic=FOOD)\n"
     ]
    }
   ],
   "source": [
    "def debugged_token(token: Token) -> str:\n",
    "    result = 'Token(%s, lemma=%s' % (token.text, token.lemma_)\n",
    "    if token._.topic is not None:\n",
    "        result += ', topic=' + token._.topic.name\n",
    "    if token._.rating is not None:\n",
    "        result += ', rating=' + token._.rating.name\n",
    "    if token._.is_dimisher:\n",
    "        result += ', dimisher'\n",
    "    if token._.is_intensifier:\n",
    "        result += ', intensifier'\n",
    "    if token._.is_negator:\n",
    "        result += ', negator'\n",
    "    result += ')'\n",
    "    return result\n",
    "\n",
    "print(debugged_token(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extend spaCy pipeline to set new attributes\n",
    "\n",
    "Now we can extend the pipeline with a step that assigns topic and rating to each token by matching it with the lexicon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opinion_matcher(doc):\n",
    "    for sentence in doc.sents:\n",
    "        for token in sentence:\n",
    "            lexicon_entry = lexicon_entry_for(token)\n",
    "            if lexicon_entry is not None:\n",
    "                if lexicon_entry.word_type is WordType.NEGATOR:\n",
    "                    token._.is_negator = True\n",
    "                elif lexicon_entry.word_type is WordType.MODIFIER:\n",
    "                    if lexicon_entry.rating.value < 0:\n",
    "                        token._.is_dimisher = True\n",
    "                    else:\n",
    "                        token._.is_intensifier = True\n",
    "                else:\n",
    "                    # Separate branch to not assign a token rating for modifiers.\n",
    "                    token._.rating = lexicon_entry.rating\n",
    "                token._.topic = lexicon_entry.topic\n",
    "    return doc\n",
    "\n",
    "if nlp.has_pipe('opinion_matcher'):\n",
    "    nlp.remove_pipe('opinion_matcher')\n",
    "nlp.add_pipe(opinion_matcher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can extract the essence for the opinions in a more integrated way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_essential(token: Token) -> bool:\n",
    "    return token._.topic is not None \\\n",
    "        or token._.rating is not None \\\n",
    "        or token._.is_dimisher or token._.is_intensifier or token._.is_negator\n",
    "        \n",
    "def essential_tokens(tokens):\n",
    "    return [token for token in tokens if is_essential(token)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token(Bratwurst, lemma=Bratwurst, topic=FOOD)\n",
      "Token(nicht, lemma=nicht, negator)\n",
      "Token(besonders, lemma=besonders, intensifier)\n",
      "Token(lecker, lemma=lecker, topic=FOOD, rating=GOOD)\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Die Bratwurst schmeckt nicht besonders lecker.')\n",
    "# Literal English: The Bratwurst tastes not especially delicious.\n",
    "\n",
    "opinion_essence = essential_tokens(doc)\n",
    "for token in opinion_essence:\n",
    "    print(debugged_token(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine tokens to ratings\n",
    "\n",
    "Now that we have all the tokens relevant to extract an opinion we have to resolve negators and modifiers.\n",
    "\n",
    "For example:\n",
    "* 'nicht schlecht' (not bad) means ``somewhat_good``\n",
    "* 'besonders gut' (especially good) means ``very_good``\n",
    "* 'nicht besonders good' (not especially good) means ``somewhat_bad``\n",
    "\n",
    "The following function scans for adjectives and applies modifiers to them while reseting the rating on the modifier tokens. First we need a few utility functions to dimish and intensify the numeric values of ``Rating``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "def signum(value):\n",
    "    if value > 0:\n",
    "        return 1\n",
    "    elif value < 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def dimished(rating_value):\n",
    "    if abs(rating_value) > 1:\n",
    "        return rating_value - signum(rating_value)\n",
    "    else:\n",
    "        return rating_value\n",
    "\n",
    "def intensified(rating_value):\n",
    "    if abs(rating_value) > 1:\n",
    "        return rating_value + signum(rating_value)\n",
    "    else:\n",
    "        return rating_value\n",
    "\n",
    "print(dimished(-2))  # Rating.BAD\n",
    "print(dimished(-1))  # Rating.SOMEWHAT_BAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-56-857fd8b6e541>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-56-857fd8b6e541>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    combined_rating_value =\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def is_rating_modifier(token):\n",
    "    return token._.is_dimisher or token._.is_intensifier or token._.is_negator\n",
    "\n",
    "def combine_ratings(tokens):\n",
    "    token_index = 0\n",
    "    while token_index < len(tokens):\n",
    "        if not is_rating_modifier(token) and (token._.rating is not None):\n",
    "            print('combine rating for %s' % token.text)\n",
    "            combined_rating_value = token._.rating.value\n",
    "            modifier_token_index = token_index - 1\n",
    "            while (modifier_token_index >= 0) and is_rating_modifier(tokens[modifier_token_index]):\n",
    "                modifier_token = tokens[modifier_token_index]\n",
    "                if modifier_token.is_dimisher:\n",
    "                    combined_rating_value = \n",
    "                \n",
    "        token_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "* Liu (2015) - Bing Liu. Sentiment Analysis. Cambridge, MA: Cambridge University Press, 2015.\n",
    "* Montani (2017) - Ines Montani. Introducing custom pipelines and extensions for spaCy v2.0. https://explosion.ai/blog/spacy-v2-pipelines-extensions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
