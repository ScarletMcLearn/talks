{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment detection with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations\n",
    "Various settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "tellers_db_path = '/tmp/tellers.db'\n",
    "lexicon_csv_path = 'lexicon_de.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import various modules we are going to need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python standard library\n",
    "import csv\n",
    "import re\n",
    "import sqlite3\n",
    "from contextlib import closing\n",
    "from enum import Enum\n",
    "\n",
    "# SpaCy\n",
    "import spacy\n",
    "from spacy.tokens import Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example feedbacks to analyze\n",
    "\n",
    "Here are a few example Resteraunteering feedbacks we are going to analyze for topic and rating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_feedback_texts = [\n",
    "    # \"The schnitzel is tasty.\"\n",
    "    'Das Schnitzel schmeckt gut.',  \n",
    "    # \"The schnitzel is terribly fat.\"\n",
    "    'Das Schnitzel ist furchtbar fett.',  \n",
    "    # \"The schnitzel should include cranberries as side dish.\"\n",
    "    'Beim Schnitzel sollten ev. Preiselbeeren als Beilage dabei sein.',\n",
    "    # \"Everything is great!\" (using an Austrian slang terms and a smiley code)\n",
    "    'Ois supa :-)',\n",
    "    # \"The football game ended 2:1\" --> unrelated to anything\n",
    "    'Das Fussballspiel endete 2:1.'          \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup texts for further processing\n",
    "Replace certain abbrevisations that would confuse spaCy when detection sentence borders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replaced 1 abbreviations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Das Schnitzel schmeckt gut.',\n",
       " 'Das Schnitzel ist furchtbar fett.',\n",
       " 'Beim Schnitzel sollten eventuell  Preiselbeeren als Beilage dabei sein.',\n",
       " 'Ois supa :-)',\n",
       " 'Das Fussballspiel endete 2:1.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ABBREVIATION_TO_EXPANDED_MAP = {\n",
    "    'ca': 'circa',  # \"approximately\"\n",
    "    'ev': 'eventuell',  # \"possibly\"\n",
    "    'max': 'maximal',\n",
    "    'vlt': 'vielleicht',  # \"maybe\"\n",
    "}\n",
    "\n",
    "def without_broken_abbreviations(feedback_texts):\n",
    "    replace_count = 0\n",
    "    for feedback_text in feedback_texts:\n",
    "        for abbreviation, expanded in ABBREVIATION_TO_EXPANDED_MAP.items():\n",
    "            # TODO: Use compiled regex.\n",
    "            previous_text = feedback_text\n",
    "            feedback_text = re.sub(\n",
    "                r'\\b' + abbreviation + r'\\.', \n",
    "                expanded + ' ',\n",
    "                feedback_text,\n",
    "                flags=re.IGNORECASE)\n",
    "            if feedback_text != previous_text:\n",
    "                replace_count += 1\n",
    "        yield feedback_text\n",
    "    print('replaced %d abbreviations' % replace_count)\n",
    "    \n",
    "feedback_texts = list(without_broken_abbreviations(original_feedback_texts))\n",
    "feedback_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a map of emojis (both western and eastern) to a distinct text form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_EMOJI_PREFIX = 'emoji__'\n",
    "_EMOJI_TO_NAME_MAP = {\n",
    "    # Western\n",
    "    ':)': 'slight_smile',\n",
    "    ':-)': 'slight_smile',\n",
    "    '=)': 'slight_smile',\n",
    "    ':(': 'slight_frown',\n",
    "    ':-(': 'slight_frown',\n",
    "    ':D': 'smile',\n",
    "    ':-D': 'smile',\n",
    "    ':P': 'stuck_out_tongue',\n",
    "    ':-P': 'stuck_out_tongue',\n",
    "    ';)': 'wink',\n",
    "    ';-)': 'wink',\n",
    "    # Eastern\n",
    "    '^^': 'slight_smile',\n",
    "    '^_^': 'slight_smile',\n",
    "}\n",
    "_EMOJI_TO_TEXT_MAP = {\n",
    "    emoji: ' ' + _EMOJI_PREFIX + name + ' '\n",
    "    for emoji, name in _EMOJI_TO_NAME_MAP.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace emojis by text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Das Schnitzel schmeckt gut.',\n",
       " 'Das Schnitzel ist furchtbar fett.',\n",
       " 'Beim Schnitzel sollten eventuell  Preiselbeeren als Beilage dabei sein.',\n",
       " 'Ois supa  emoji__slight_smile ',\n",
       " 'Das Fussballspiel endete 2:1.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def feedback_texts_with_unified_emojiis(feedback_texts):\n",
    "    for feedback_text in feedback_texts:\n",
    "        for emoji, emoji_text in _EMOJI_TO_TEXT_MAP.items():\n",
    "            feedback_text= feedback_text.replace(emoji, emoji_text)\n",
    "        yield feedback_text\n",
    "            \n",
    "\n",
    "feedback_texts = list(feedback_texts_with_unified_emojiis(feedback_texts))\n",
    "feedback_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace some Austrian slang term by proper German:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replaced 2 austrian slang terms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Das Schnitzel schmeckt gut.',\n",
       " 'Das Schnitzel ist furchtbar fett.',\n",
       " 'Beim Schnitzel sollten eventuell  Preiselbeeren als Beilage dabei sein.',\n",
       " 'alles  super   emoji__slight_smile ',\n",
       " 'Das Fussballspiel endete 2:1.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_AUSTRIAN_TO_GERMAN_SYNONYM_MAP = {\n",
    "    'eh': 'ohnehin',\n",
    "    'nix': 'nichts',\n",
    "    'ois': 'alles',\n",
    "    'supa': 'super',\n",
    "}\n",
    "\n",
    "def feedback_texts_without_austrian_slang(feedback_texts):\n",
    "    replace_count = 0\n",
    "    for feedback_text in feedback_texts:\n",
    "        for austrian_word, german_word in _AUSTRIAN_TO_GERMAN_SYNONYM_MAP.items():\n",
    "            # TODO: Use compiled regex.\n",
    "            previous_text = feedback_text\n",
    "            feedback_text = re.sub(\n",
    "                r'\\b' + austrian_word + r'\\b', \n",
    "                german_word + ' ',\n",
    "                feedback_text,\n",
    "                flags=re.IGNORECASE)\n",
    "            if feedback_text != previous_text:\n",
    "                replace_count += 1\n",
    "        yield feedback_text\n",
    "    print('replaced %d austrian slang terms' % replace_count)\n",
    "    \n",
    "feedback_texts = list(feedback_texts_without_austrian_slang(feedback_texts))\n",
    "feedback_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into sentences\n",
    "Because we are working with German texts, we need to load the German language into spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('de')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy can split long documents into sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Der Keller war sehr freundlich.\n",
      "Die Wartezeiten sind aber viel zu lang.\n"
     ]
    }
   ],
   "source": [
    "document = nlp('Der Keller war sehr freundlich. Die Wartezeiten sind aber viel zu lang.')\n",
    "#              The waiter was very friendly.    Waiting times are way too long though\n",
    "for tokens in document.sents:\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics\n",
    "\n",
    "There are several ways to find appropriate topics. We are going to use:\n",
    "\n",
    "* ambience: decoration, space, light, temperature, ...\n",
    "* food and beverages: eating, drinking, taste, menu, selection\n",
    "* hygiene: toilett, smell, ...\n",
    "* service: waiting and reaction times, courtesy, competence, availability, ...\n",
    "* value: size of portions, price, ...\n",
    "\n",
    "This can be represented as Python Enum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Topic(Enum):\n",
    "    UNKNWON = -99\n",
    "    AMBIENCE = 1\n",
    "    FOOD = 2\n",
    "    HYGIENE = 3\n",
    "    SERVICE = 4\n",
    "    VALUE = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rating\n",
    "\n",
    "There are serverl ways to represent a rating:\n",
    "\n",
    "* Use \"prositive\" and \"negative\"\n",
    "* Same as above but with more distincz values, e.g. 1 to 5 stars\n",
    "* use a float between e.g. 0 and 1.0\n",
    "\n",
    "We are going to use a system with 6 discret values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rating(Enum):\n",
    "    UNKNOWN = -99\n",
    "    VERY_BAD = -4\n",
    "    BAD = -2\n",
    "    SOMEWHAT_BAD = -1\n",
    "    SOMEWHAT_GOOD = 1\n",
    "    GOOD = 2\n",
    "    VERY_GOOD = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicon\n",
    "\n",
    "In a lexicon based sentiment analysis, a lexicon collects words and assigns them to topics and ratings. It also includes more information a parser can utilize to combine multiple words.\n",
    "\n",
    "Different types of word have various implications on how an opinion can be extracted. A simple system that works with simple sentences:\n",
    "\n",
    "* noun: Schnitzel, Bier (beer), Geruch (smell)\n",
    "* adjective: toll (great), entäuschend (disappointing), wohlschmeckend (tasty)\n",
    "* verb: warten (to wait), stinken (to smell)\n",
    "* modifiers (intensify or dimish adjective): eher (somewhat), besonders (very), zu (too), viel zu (much to)\n",
    "* negator: nicht (not), kein\n",
    "\n",
    "Negators can also be prefixes like \"un\" and \"in\", e.g. brauchbar (suitable) - unbrauchbar (unfit) or kompetent (competent) - inkompetent (incompetent).\n",
    "\n",
    "Be aware that negators do not simply change the sign of a rating, for example:\n",
    "* \"schlecht\" (bad) - BAD\n",
    "* \"nicht schlecht\" (not bad) - SOMEWHAT_GOOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordType(Enum):\n",
    "    UNKNOWN = -99\n",
    "    NOUN = 1\n",
    "    ADJECTIVE = 2\n",
    "    VERB = 3\n",
    "    MODIFIER = 4\n",
    "    NEGATOR = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words can also be regular expressions to reduce the size. For example, various types of wine (\"Rotwein\", \"Weißwein\", \"Portwein\") can be reduced to the regular expression `r'.*wein'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lexicon entry combines all this information for each word. It also has a ``matching()`` function to compute how close a token is related to a lexicon entry. This does not perform any semantic analysis, it's just a simple string comparison or regex match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LexiconEntry():\n",
    "    _IS_REGEX_REGEX = re.compile(r'.*[.+*\\[$^\\\\]')\n",
    "    \n",
    "    def __init__(self, lemma: str, word_type: WordType, topic: Topic, rating: Rating):\n",
    "        assert rating is not None if word_type is WordType.MODIFIER else True, 'modifier must have rating: ' + lemma\n",
    "        self.lemma = lemma\n",
    "        self._lower_lemma = lemma.lower()\n",
    "        self.word_type = word_type\n",
    "        self.topic = topic\n",
    "        self.rating = rating\n",
    "        self.is_regex = LexiconEntry._IS_REGEX_REGEX.match(self.lemma) is not None       \n",
    "        self._regex = re.compile(lemma) if self.is_regex else None\n",
    "    \n",
    "    def matching(self, token: Token) -> float:\n",
    "        result = 0.0\n",
    "        if self.is_regex:\n",
    "            if self._regex.match(token.text):\n",
    "                result = 0.6\n",
    "            elif self._regex.match(token.lemma_):\n",
    "                result = 0.5\n",
    "        else:\n",
    "            if token.text == self.lemma:\n",
    "                result = 1.0\n",
    "            elif token.text.lower() == self.lemma:\n",
    "                result = 0.9\n",
    "            elif token.lemma_ == self.lemma:\n",
    "                result = 0.8\n",
    "            elif token.lemma_.lower() == self.lemma:\n",
    "                result = 0.7\n",
    "        return result\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        result = 'LexiconEntry(%s, word_type=%s' % (self.lemma, self.word_type.name)\n",
    "        if self.topic is not None:\n",
    "            result += ', topic=%s' % self.topic.name\n",
    "        if self.rating is not None:\n",
    "            result += ', rating=%s' % self.rating.name\n",
    "        if self.is_regex:\n",
    "            result += ', is_regex=%s' % self.is_regex\n",
    "        result += ')'\n",
    "        return result\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on this approach, see Liu (2015, p. 59ff)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing and reading the lexicon\n",
    "\n",
    "A simple way to store the lexicon is a CSV file with columns for:\n",
    "\n",
    "* lemma or pattern\n",
    "* word type\n",
    "* topic\n",
    "* rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 474 lexicon entries\n"
     ]
    }
   ],
   "source": [
    "_RATING_NAME_TO_VALUE_MAP = {some.name.lower(): some for some in Rating}\n",
    "_TOPIC_NAME_TO_VALUE_MAP = {some.name.lower(): some for some in Topic}\n",
    "_WORD_TYPE_NAME_TO_VALUE_MAP = {some.name.lower(): some for some in WordType}\n",
    "\n",
    "lexicon = []\n",
    "with open(lexicon_csv_path, encoding='utf-8', newline='') as lexicon_file:\n",
    "    lexicon_reader = csv.reader(lexicon_file, delimiter=',')\n",
    "    for row in lexicon_reader:\n",
    "        row = [item.strip() for item in row]\n",
    "        row += 4 * ['']  # Ensure we have at least 4 strings\n",
    "        lemma, word_type_text, topic_text, rating_text = row[:4]\n",
    "        if lemma != '' and not lemma.startswith('#'):\n",
    "            try:\n",
    "                # Map certain columns to enums.\n",
    "                word_type = _WORD_TYPE_NAME_TO_VALUE_MAP[word_type_text]\n",
    "                topic = _TOPIC_NAME_TO_VALUE_MAP.get(topic_text)\n",
    "                rating = _RATING_NAME_TO_VALUE_MAP.get(rating_text)\n",
    "            except KeyError as error:\n",
    "                raise csv.Error(\n",
    "                    '%s:%d: cannot map value: %s' % (\n",
    "                        lexicon_csv_path, lexicon_reader.line_num, error))\n",
    "            lexicon_entry = LexiconEntry(lemma, word_type, topic, rating)\n",
    "            lexicon.append(lexicon_entry)\n",
    "print('found %d lexicon entries' % len(lexicon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find base word (lemma) for token\n",
    "\n",
    "This can again be done with the help of SpaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Der -> Der\n",
      "Kellner -> kellnern\n",
      "war -> sein\n",
      "diesmal -> diesmal\n",
      "freundlicher -> freundlich\n",
      "als -> als\n",
      "zuletzt -> zuletzt\n",
      ". -> .\n"
     ]
    }
   ],
   "source": [
    "document = nlp('Der Kellner war diesmal freundlicher als zuletzt.')\n",
    "#               This time the waiter was more friendly than the last time.\n",
    "for tokens in document.sents:\n",
    "    for token in tokens:\n",
    "        print('%s -> %s' % (token.text, token.lemma_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match lemma with lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LexiconEntry(lecker, word_type=ADJECTIVE, topic=FOOD, rating=GOOD)\n"
     ]
    }
   ],
   "source": [
    "def lexicon_entry_for(token) -> LexiconEntry:\n",
    "    result = None\n",
    "    lemma = token.lemma_\n",
    "    lower_lemma = lemma.lower()\n",
    "    lexicon_entry_index = 0\n",
    "    best_matching = 0.0\n",
    "    # TODO: Improve performance by not having to scan whole lexicon for each token.\n",
    "    for lexicon_entry in lexicon:\n",
    "        matching = lexicon_entry.matching(token)\n",
    "        if matching > best_matching:\n",
    "            result = lexicon_entry\n",
    "            best_matching = matching\n",
    "    return result\n",
    "\n",
    "token = next(nlp('lecker').sents)\n",
    "lecker_entry = LexiconEntry('lecker', WordType.ADJECTIVE, Topic.FOOD, Rating.GOOD)\n",
    "print(lexicon_entry_for(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we cann reduce opinion senteces to lists of topics and ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LexiconEntry(Bratwurst, word_type=NOUN, topic=FOOD)\n",
      "LexiconEntry(lecker, word_type=ADJECTIVE, topic=FOOD, rating=GOOD)\n"
     ]
    }
   ],
   "source": [
    "opinion_essence = []\n",
    "for token in nlp('Die Bratwurst schmeckt sehr lecker!'):\n",
    "    matching_lexicon_entry = lexicon_entry_for(token)\n",
    "    if matching_lexicon_entry is not None:\n",
    "        opinion_essence.append(matching_lexicon_entry)\n",
    "for lexicon_entry in opinion_essence:\n",
    "    print(lexicon_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add spaCy extensions for topic, rating, etc\n",
    "\n",
    "SpaCy has an extension API to store (among other things) additional attributes on documents, spans and tokens (see Montani (2017)). To add attributes for topic and rating we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Token.set_extension('topic', default=None)\n",
    "Token.set_extension('rating', default=None)\n",
    "Token.set_extension('is_negator', default=False)\n",
    "Token.set_extension('is_intensifier', default=False)\n",
    "Token.set_extension('is_dimisher', default=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now set and get these attributes using for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bratwurst\n",
      "Topic.FOOD\n"
     ]
    }
   ],
   "source": [
    "token = next(nlp('Bratwurst').sents)[0]\n",
    "print(token.lemma_)\n",
    "token._.topic = Topic.FOOD\n",
    "print(token._.topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify debugging the following function shows the Token and its relevant attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token(Bratwurst, lemma=Bratwurst, topic=FOOD)\n"
     ]
    }
   ],
   "source": [
    "def debugged_token(token: Token) -> str:\n",
    "    result = 'Token(%s, lemma=%s' % (token.text, token.lemma_)\n",
    "    if token._.topic is not None:\n",
    "        result += ', topic=' + token._.topic.name\n",
    "    if token._.rating is not None:\n",
    "        result += ', rating=' + token._.rating.name\n",
    "    if token._.is_dimisher:\n",
    "        result += ', dimisher'\n",
    "    if token._.is_intensifier:\n",
    "        result += ', intensifier'\n",
    "    if token._.is_negator:\n",
    "        result += ', negator'\n",
    "    result += ')'\n",
    "    return result\n",
    "\n",
    "print(debugged_token(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extend spaCy pipeline to set new attributes\n",
    "\n",
    "Now we can extend the pipeline with a step that assigns topic and rating to each token by matching it with the lexicon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opinion_matcher(doc):\n",
    "    for sentence in doc.sents:\n",
    "        for token in sentence:\n",
    "            lexicon_entry = lexicon_entry_for(token)\n",
    "            if lexicon_entry is not None:\n",
    "                if lexicon_entry.word_type is WordType.NEGATOR:\n",
    "                    token._.is_negator = True\n",
    "                elif lexicon_entry.word_type is WordType.MODIFIER:\n",
    "                    if lexicon_entry.rating.value < 0:\n",
    "                        token._.is_dimisher = True\n",
    "                    else:\n",
    "                        token._.is_intensifier = True\n",
    "                else:\n",
    "                    # Separate branch to not assign a token rating for modifiers.\n",
    "                    token._.rating = lexicon_entry.rating\n",
    "                token._.topic = lexicon_entry.topic\n",
    "    return doc\n",
    "\n",
    "if nlp.has_pipe('opinion_matcher'):\n",
    "    nlp.remove_pipe('opinion_matcher')\n",
    "nlp.add_pipe(opinion_matcher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can extract the essence for the opinions in a more integrated way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_essential(token: Token) -> bool:\n",
    "    return token._.topic is not None \\\n",
    "        or token._.rating is not None \\\n",
    "        or token._.is_dimisher or token._.is_intensifier or token._.is_negator\n",
    "        \n",
    "def essential_tokens(tokens):\n",
    "    return [token for token in tokens if is_essential(token)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token(Bratwurst, lemma=Bratwurst, topic=FOOD)\n",
      "Token(nicht, lemma=nicht, negator)\n",
      "Token(besonders, lemma=besonders, intensifier)\n",
      "Token(lecker, lemma=lecker, topic=FOOD, rating=GOOD)\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Die Bratwurst schmeckt nicht besonders lecker.')\n",
    "# Literal English: The Bratwurst tastes not especially delicious.\n",
    "\n",
    "opinion_essence = essential_tokens(doc)\n",
    "for token in opinion_essence:\n",
    "    print(debugged_token(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine tokens to ratings\n",
    "\n",
    "Now that we have all the tokens relevant to extract an opinion we have to resolve negators and modifiers.\n",
    "\n",
    "For example:\n",
    "* 'nicht schlecht' (not bad) means ``somewhat_good``\n",
    "* 'besonders gut' (especially good) means ``very_good``\n",
    "* 'nicht besonders good' (not especially good) means ``somewhat_bad``\n",
    "\n",
    "The following function scans for adjectives and applies modifiers to them while reseting the rating on the modifier tokens. First we need a few utility functions to dimish and intensify the numeric values of ``Rating``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "def signum(value):\n",
    "    if value > 0:\n",
    "        return 1\n",
    "    elif value < 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def dimished(rating_value):\n",
    "    if abs(rating_value) > 1:\n",
    "        return rating_value - signum(rating_value)\n",
    "    else:\n",
    "        return rating_value\n",
    "\n",
    "def intensified(rating_value):\n",
    "    if abs(rating_value) > 1:\n",
    "        return rating_value + signum(rating_value)\n",
    "    else:\n",
    "        return rating_value\n",
    "\n",
    "print(dimished(-2))  # Rating.BAD\n",
    "print(dimished(-1))  # Rating.SOMEWHAT_BAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a function to convert numeric ratings back to the ``Rating`` enum that can also deal with values out side of the valid range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating.SOMEWHAT_BAD\n",
      "Rating.VERY_GOOD\n"
     ]
    }
   ],
   "source": [
    "def rating_for_value(rating_value: int) -> Rating:\n",
    "    result = None\n",
    "    if rating_value <= Rating.VERY_BAD.value:\n",
    "        result = Rating.VERY_BAD\n",
    "    elif rating_value <= Rating.BAD.value:\n",
    "        result = Rating.BAD\n",
    "    elif rating_value <= Rating.SOMEWHAT_BAD.value:\n",
    "        result = Rating.SOMEWHAT_BAD\n",
    "    elif rating_value >= Rating.VERY_GOOD.value:\n",
    "        result = Rating.VERY_GOOD\n",
    "    elif rating_value >= Rating.GOOD.value:\n",
    "        result = Rating.GOOD\n",
    "    elif rating_value >= Rating.SOMEWHAT_GOOD.value:\n",
    "        result = Rating.SOMEWHAT_GOOD\n",
    "    return result\n",
    "\n",
    "print(rating_for_value(-1))\n",
    "print(rating_for_value(9001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_rating_modifier(token: Token):\n",
    "    return token._.is_dimisher or token._.is_intensifier or token._.is_negator\n",
    "\n",
    "def combine_ratings(tokens):\n",
    "    token_index = 0\n",
    "    while token_index < len(tokens):\n",
    "        token = tokens[token_index]\n",
    "        if not is_rating_modifier(token) and (token._.rating is not None):\n",
    "            # print('    combine rating for %s' % token.text)\n",
    "            initial_rating_value = token._.rating.value\n",
    "            combined_rating_value = initial_rating_value\n",
    "            modifier_token_index = token_index - 1\n",
    "            # print('1: ', tokens[modifier_token_index])\n",
    "            while (modifier_token_index >= 0) and is_rating_modifier(tokens[modifier_token_index]):\n",
    "                # print('2: ', tokens[modifier_token_index])\n",
    "                modifier_token = tokens[modifier_token_index]\n",
    "                applied = True\n",
    "                if modifier_token._.is_dimisher:\n",
    "                    # print('      dimish: %s' % modifier_token.text)\n",
    "                    combined_rating_value = dimished(combined_rating_value)\n",
    "                elif modifier_token._.is_intensifier:\n",
    "                    # print('      intensify: %s' % modifier_token.text)\n",
    "                    combined_rating_value = intensified(combined_rating_value)\n",
    "                elif modifier_token._.is_negator:\n",
    "                    # print('      negate+dimish: %s' % modifier_token.text)\n",
    "                    combined_rating_value = -dimished(combined_rating_value)\n",
    "                else:\n",
    "                    applied = False\n",
    "                if applied:\n",
    "                    tokens.remove(modifier_token)\n",
    "                modifier_token_index -= 1\n",
    "            if combined_rating_value != initial_rating_value:\n",
    "                # print('      %s: %s --> %s' % (token.text, initial_rating_value, combined_rating_value))\n",
    "                token._.rating = rating_for_value(combined_rating_value)\n",
    "        token_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with our example from before we can now do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "essential tokens:\n",
      "   Token(Bratwurst, lemma=Bratwurst, topic=FOOD)\n",
      "   Token(nicht, lemma=nicht, negator)\n",
      "   Token(besonders, lemma=besonders, intensifier)\n",
      "   Token(lecker, lemma=lecker, topic=FOOD, rating=GOOD)\n",
      "combined tokens:\n",
      "   Token(Bratwurst, lemma=Bratwurst, topic=FOOD)\n",
      "   Token(lecker, lemma=lecker, topic=FOOD, rating=BAD)\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Die Bratwurst schmeckt nicht besonders lecker.')\n",
    "# Literal English: The Bratwurst tastes not especially delicious.\n",
    "\n",
    "opinion_essence = essential_tokens(doc)\n",
    "print('essential tokens:')\n",
    "for token in opinion_essence:\n",
    "    print('  ', debugged_token(token))\n",
    "\n",
    "combine_ratings(opinion_essence)\n",
    "print('combined tokens:')\n",
    "for token in opinion_essence:\n",
    "    print('  ', debugged_token(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A function for topic and rating\n",
    "Now that we have all the building blocks, we can move this into a single functions that extracts the first topic and rating of a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_and_rating_for(text: str):\n",
    "    result_topic = None\n",
    "    result_rating = None\n",
    "    doc = nlp(text)\n",
    "    opinion_essence = essential_tokens(doc)\n",
    "    # print('  1: ', opinion_essence)\n",
    "    combine_ratings(opinion_essence)\n",
    "    # print('  2: ', opinion_essence)\n",
    "    for token in opinion_essence:\n",
    "        # print(debugged_token(token))\n",
    "        if (token._.topic is not None) and (result_topic is None):\n",
    "            result_topic = token._.topic\n",
    "        if (token._.rating is not None) and (result_rating is None):\n",
    "            result_rating = token._.rating\n",
    "        if (result_topic is not None) and (result_rating is not None):\n",
    "            break\n",
    "    return result_topic, result_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can exercise this with a few test sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das Schnitzel schmeckt gut.\n",
      "  ->  (<Topic.FOOD: 2>, <Rating.GOOD: 2>)\n",
      "Das Schnitzel ist furchtbar fett.\n",
      "  ->  (<Topic.FOOD: 2>, <Rating.VERY_BAD: -4>)\n",
      "Beim Schnitzel sollten eventuell  Preiselbeeren als Beilage dabei sein.\n",
      "  ->  (<Topic.FOOD: 2>, <Rating.SOMEWHAT_BAD: -1>)\n",
      "alles  super   emoji__slight_smile \n",
      "  ->  (None, <Rating.VERY_GOOD: 4>)\n",
      "Das Fussballspiel endete 2:1.\n",
      "  ->  (None, None)\n"
     ]
    }
   ],
   "source": [
    "for feedback_text in feedback_texts:\n",
    "    print(feedback_text)\n",
    "    print('  -> ', topic_and_rating_for(feedback_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "* Liu (2015) - Bing Liu. Sentiment Analysis. Cambridge, MA: Cambridge University Press, 2015.\n",
    "* Montani (2017) - Ines Montani. Introducing custom pipelines and extensions for spaCy v2.0. https://explosion.ai/blog/spacy-v2-pipelines-extensions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
